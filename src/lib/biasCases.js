// src/lib/biasCases.js
// Static case data for the AI Bias Detective activity.
// Ported from the standalone AI Bias Detective app.

export const CASES = [
  {
    id: "hireright",
    title: "HireRight AI",
    subtitle: "Resume Screening Algorithm",
    description:
      "TechForward Inc. deployed an AI system to screen job applications for software engineering positions. Several candidates have filed complaints alleging unfair treatment. Your job: investigate the training data, identify any biases, and recommend fixes.",
    icon: "briefcase",
    emoji: "\u{1F4BC}",
    difficulty: 1,
    difficultyLabel: "Beginner",
    totalClues: 6,
    estimatedTime: "25-35 min",
    color: "var(--cyan, #22d3ee)",
    aiSystem: {
      name: "HireRight v2.3",
      purpose: "Automated resume screening for software engineering roles",
      deployer: "TechForward Inc.",
      usageSince: "March 2023",
      applicationsProcessed: "12,847",
    },
    trainingData: {
      description:
        "The system was trained on 5 years of hiring decisions (2018-2023) from TechForward's existing workforce.",
      demographics: [
        {
          category: "Gender in Training Data",
          breakdown: [
            { label: "Male", value: 78, color: "#60a5fa" },
            { label: "Female", value: 18, color: "#f472b6" },
            { label: "Non-binary", value: 4, color: "#a78bfa" },
          ],
        },
        {
          category: "Race/Ethnicity in Training Data",
          breakdown: [
            { label: "White", value: 62, color: "#60a5fa" },
            { label: "Asian", value: 24, color: "#34d399" },
            { label: "Hispanic/Latino", value: 8, color: "#f5a623" },
            { label: "Black", value: 4, color: "#f472b6" },
            { label: "Other", value: 2, color: "#a78bfa" },
          ],
        },
        {
          category: "Age Distribution in Training Data",
          breakdown: [
            { label: "22-30", value: 35, color: "#34d399" },
            { label: "31-40", value: 42, color: "#60a5fa" },
            { label: "41-50", value: 18, color: "#f5a623" },
            { label: "51+", value: 5, color: "#f472b6" },
          ],
        },
      ],
      sampleRecords: [
        { id: "r1", name: "James Mitchell", score: 92, result: "ACCEPTED", college: "Stanford", gradYear: 2019, gap: false, zipCode: "94301", skills: "Python, React, AWS", yearsExp: 4 },
        { id: "r2", name: "Maria Gonzalez", score: 61, result: "REJECTED", college: "UT Austin", gradYear: 2018, gap: true, zipCode: "78745", skills: "Python, Java, Docker", yearsExp: 5 },
        { id: "r3", name: "Wei Chen", score: 88, result: "ACCEPTED", college: "MIT", gradYear: 2020, gap: false, zipCode: "02139", skills: "Python, C++, ML", yearsExp: 3 },
        { id: "r4", name: "Aisha Johnson", score: 55, result: "REJECTED", college: "Howard University", gradYear: 2017, gap: false, zipCode: "20059", skills: "Java, Python, SQL, React", yearsExp: 6 },
        { id: "r5", name: "Robert Smith", score: 85, result: "ACCEPTED", college: "Georgia Tech", gradYear: 2021, gap: false, zipCode: "30332", skills: "JavaScript, React", yearsExp: 2 },
        { id: "r6", name: "Priya Patel", score: 58, result: "REJECTED", college: "UC Berkeley", gradYear: 2015, gap: true, zipCode: "94704", skills: "Python, ML, TensorFlow, AWS", yearsExp: 8 },
        { id: "r7", name: "John O'Brien", score: 79, result: "ACCEPTED", college: "Boston College", gradYear: 2020, gap: false, zipCode: "02467", skills: "JavaScript, Node.js", yearsExp: 3 },
        { id: "r8", name: "DeShawn Williams", score: 52, result: "REJECTED", college: "Morehouse College", gradYear: 2019, gap: false, zipCode: "30314", skills: "Python, Java, React, AWS, Docker", yearsExp: 4 },
        { id: "r9", name: "Sarah Kim", score: 63, result: "REJECTED", college: "Carnegie Mellon", gradYear: 2012, gap: true, zipCode: "15213", skills: "C++, Python, ML, Management", yearsExp: 11 },
        { id: "r10", name: "Michael Johnson", score: 81, result: "ACCEPTED", college: "Purdue", gradYear: 2022, gap: false, zipCode: "47907", skills: "Python, JavaScript", yearsExp: 1 },
      ],
      recordColumns: [
        { key: "name", label: "Name", align: "left" },
        { key: "score", label: "Score" },
        { key: "result", label: "Result", badge: true },
        { key: "college", label: "College" },
        { key: "gradYear", label: "Grad Year" },
        { key: "gap", label: "Gap", boolean: true },
        { key: "zipCode", label: "Zip" },
        { key: "skills", label: "Skills", truncate: 120 },
        { key: "yearsExp", label: "Yrs Exp" },
      ],
      featureWeights: [
        { feature: "University Prestige Ranking", weight: 0.28, suspicious: true },
        { feature: "Years of Experience", weight: 0.15, suspicious: false },
        { feature: "Technical Skills Match", weight: 0.12, suspicious: false },
        { feature: "Graduation Year (Recency)", weight: 0.18, suspicious: true },
        { feature: "Career Gaps", weight: 0.14, suspicious: true },
        { feature: "Zip Code Region Score", weight: 0.08, suspicious: true },
        { feature: "Keyword Density", weight: 0.05, suspicious: false },
      ],
      ratesLabel: "% of Applicants Hired by Group",
      approvalRates: [
        { group: "Male applicants", rate: 42 },
        { group: "Female applicants", rate: 19 },
        { group: "Non-binary applicants", rate: 11 },
        { group: "White applicants", rate: 44 },
        { group: "Black applicants", rate: 12 },
        { group: "Hispanic applicants", rate: 18 },
        { group: "Asian applicants", rate: 38 },
        { group: "Age 22-30", rate: 48 },
        { group: "Age 31-40", rate: 35 },
        { group: "Age 41-50", rate: 15 },
        { group: "Age 51+", rate: 8 },
        { group: "No career gaps", rate: 41 },
        { group: "Has career gap", rate: 14 },
      ],
    },
    dataRoomQuestions: {
      demographics: { question: "What is the most concerning pattern in this training data?", choices: [{ id: "a", text: "There aren't enough data points overall" }, { id: "b", text: "The data heavily overrepresents male candidates (78%)" }, { id: "c", text: "The age groups are evenly distributed" }, { id: "d", text: "There are too many racial categories" }], correctAnswer: "b", feedbackCorrect: "Correct! When 78% of training data is male, the AI learns to associate 'successful candidate' primarily with male characteristics.", feedbackIncorrect: "Look more carefully at the gender breakdown. When one group makes up 78% of the data, the AI's definition of 'good candidate' becomes skewed toward that group." },
      sampleRecords: { question: "Comparing the sample records, which pattern most strongly suggests unfair evaluation?", choices: [{ id: "a", text: "Some candidates have more skills listed than others" }, { id: "b", text: "Rejected candidates tend to have non-Anglo names" }, { id: "c", text: "Candidates with more experience and skills are sometimes rejected while less-qualified candidates are accepted" }, { id: "d", text: "All rejected candidates went to bad schools" }], correctAnswer: "c", feedbackCorrect: "Right! Aisha Johnson (6yr exp, 4 skills, rejected) vs Robert Smith (2yr exp, 2 skills, accepted) is a clear red flag that the scoring isn't based on merit alone.", feedbackIncorrect: "Compare Aisha Johnson (Howard, 6 years exp, 4 skills, REJECTED with score 55) to Robert Smith (Georgia Tech, 2 years exp, 2 skills, ACCEPTED with score 85). What could explain this?" },
      featureWeights: { question: "Which feature weight assignment is most problematic?", choices: [{ id: "a", text: "Years of Experience at 0.15 is too low" }, { id: "b", text: "University Prestige at 0.28 is the highest weight \u2014 higher than actual technical skills at 0.12" }, { id: "c", text: "Keyword Density at 0.05 is useless" }, { id: "d", text: "All weights seem reasonable for hiring" }], correctAnswer: "b", feedbackCorrect: "Exactly! When where you went to school matters 2.3x more than your actual technical abilities, the system rewards privilege over merit.", feedbackIncorrect: "Look at the top-weighted feature: University Prestige (0.28) outweighs Technical Skills Match (0.12) by more than 2x. Should where you went to school matter more than what you can actually do?" },
      approvalRates: { question: "What does the approval rate data reveal?", choices: [{ id: "a", text: "The system treats all demographic groups fairly" }, { id: "b", text: "Age is the only factor that matters" }, { id: "c", text: "There are dramatic disparities across gender, race, and age \u2014 suggesting systemic bias" }, { id: "d", text: "Career gaps are the main issue" }], correctAnswer: "c", feedbackCorrect: "Correct! Male 42% vs Female 19%, White 44% vs Black 12%, Age 22-30 at 48% vs 51+ at 8% \u2014 these are massive disparities across multiple dimensions.", feedbackIncorrect: "Compare the rates across ALL groups: Male 42% vs Female 19%, White 44% vs Black 12%, Age 22-30 at 48% vs 51+ at 8%. These aren't minor differences \u2014 they reveal systemic patterns." },
    },
    clues: [
      { id: "hr-clue-1", title: "Gender Imbalance in Training Data", category: "Training Data", description: "The training data contains 78% male resumes. The AI learned from a workforce that was already skewed, meaning it treats 'successful candidate' as more likely to be male.", evidence: "Gender demographics chart shows 78% male, 18% female, 4% non-binary in training set.", biasType: "Historical Bias / Representation Bias", severity: "high", points: 8, question: { evidenceSnippet: "The training data contains 78% male resumes, with only 18% female and 4% non-binary representation.", prompt: "What is the primary risk when an AI is trained on such imbalanced data?", choices: [{ id: "a", text: "The AI will only hire male candidates" }, { id: "b", text: "The AI learns to associate 'success' with male characteristics, systematically disadvantaging other genders" }, { id: "c", text: "The AI won't be able to read female resumes" }, { id: "d", text: "There's no real risk since gender shouldn't affect job skills" }], correctAnswer: "b", feedbackCorrect: "Exactly! The AI doesn't explicitly filter by gender \u2014 it subtly learns that patterns associated with male candidates (language, schools, career paths) equal 'good candidate.'", feedbackWrong: { a: "The AI doesn't have a hard gender filter \u2014 the bias is more subtle than outright exclusion. It learns patterns associated with the dominant group.", c: "The AI can process all resumes technically. The problem is in how it SCORES them based on patterns it learned from imbalanced data.", d: "You're right that gender shouldn't affect skills, but that's exactly the problem \u2014 the AI has learned gender-correlated patterns that ARE affecting scores." } } },
      { id: "hr-clue-2", title: "Career Gap Penalty", category: "Feature Analysis", description: "Career gaps receive a 14% feature weight penalty. Career gaps disproportionately affect women (maternity leave), caregivers, and people with health conditions. This is a proxy for gender discrimination.", evidence: "Feature weights show 'Career Gaps' at 0.14 weight. Approval rate for candidates with gaps: 14% vs 41% without.", biasType: "Proxy Discrimination", severity: "high", points: 8, question: { evidenceSnippet: "Career gaps carry a 14% weight penalty. Candidates with gaps have a 14% approval rate vs 41% without gaps.", prompt: "Why is penalizing career gaps a form of bias?", choices: [{ id: "a", text: "Career gaps always indicate laziness" }, { id: "b", text: "Employers should never consider employment history" }, { id: "c", text: "Career gaps disproportionately affect women (maternity), caregivers, and people with health conditions \u2014 making this a proxy for protected characteristics" }, { id: "d", text: "The 14% weight is just too mathematically high" }], correctAnswer: "c", feedbackCorrect: "Correct! Career gaps are a textbook example of proxy discrimination \u2014 a seemingly neutral factor that disproportionately impacts specific protected groups.", feedbackWrong: { a: "Career gaps happen for many valid reasons: raising children, caring for sick family members, health conditions, education, military service. Penalizing them punishes life circumstances that correlate with gender and disability.", b: "Employment history can be relevant, but the REASON for gaps matters. Blanket penalties punish groups who are more likely to have gaps for reasons beyond their control.", d: "The issue isn't just the mathematical weight \u2014 it's that ANY penalty on career gaps creates disparate impact on women, caregivers, and people with disabilities." } } },
      { id: "hr-clue-3", title: "University Prestige Overweighted", category: "Feature Analysis", description: "University prestige is the #1 factor at 28% weight \u2014 higher than actual technical skills (12%). This disadvantages candidates from HBCUs, community colleges, and state schools, correlating with race and socioeconomic status.", evidence: "Feature weights: University Prestige 0.28 vs Technical Skills 0.12. Compare Aisha (Howard, rejected, 6yr exp) vs Robert (Georgia Tech, accepted, 2yr exp).", biasType: "Structural / Institutional Bias", severity: "high", points: 7, question: { evidenceSnippet: "University prestige is weighted at 0.28 \u2014 more than double the weight of technical skills (0.12). Compare Aisha (Howard University, 6yr exp, rejected) vs Robert (Georgia Tech, 2yr exp, accepted).", prompt: "Why does overweighting university prestige create bias?", choices: [{ id: "a", text: "Prestigious universities actually do produce better engineers" }, { id: "b", text: "It discriminates against candidates from HBCUs, community colleges, and state schools \u2014 which correlates with race and socioeconomic status" }, { id: "c", text: "University quality doesn't matter at all for job performance" }, { id: "d", text: "It only affects international students" }], correctAnswer: "b", feedbackCorrect: "Right! Access to prestigious universities is heavily influenced by race, wealth, and family connections. Using prestige as the top factor launders socioeconomic privilege into 'merit.'", feedbackWrong: { a: "Even if some prestigious schools have strong programs, making it the TOP factor (above skills and experience) means a Harvard grad with no experience outranks a Howard grad with 6 years. That's not measuring ability.", c: "University quality can be one factor, but when it's weighted 2.3x more than actual technical skills, it becomes a proxy for privilege rather than ability.", d: "It affects many domestic groups too \u2014 students from HBCUs, community colleges, rural state schools, and anyone who couldn't afford or access elite institutions." } } },
      { id: "hr-clue-4", title: "Age Discrimination via Graduation Year", category: "Feature Analysis", description: "Graduation year recency has an 18% weight, heavily favoring recent graduates. Older workers with more experience are systematically penalized. Compare Sarah Kim (CMU, 11yr exp, rejected) to Michael Johnson (Purdue, 1yr exp, accepted).", evidence: "Approval rates drop sharply with age: 48% (22-30), 35% (31-40), 15% (41-50), 8% (51+).", biasType: "Age Discrimination", severity: "medium", points: 7, question: { evidenceSnippet: "Graduation year recency has an 18% weight. Approval rates: Age 22-30 at 48%, Age 31-40 at 35%, Age 41-50 at 15%, Age 51+ at just 8%.", prompt: "What type of bias does weighting graduation recency create?", choices: [{ id: "a", text: "It's not biased \u2014 recent graduates have more current skills" }, { id: "b", text: "Age discrimination \u2014 it systematically penalizes older workers regardless of their actual abilities and experience" }, { id: "c", text: "It only affects people who went back to school later" }, { id: "d", text: "Educational bias against certain degree types" }], correctAnswer: "b", feedbackCorrect: "Correct! Graduation recency is a direct proxy for age. The steep decline from 48% (age 22-30) to 8% (51+) shows the system treats older workers as less desirable despite their experience.", feedbackWrong: { a: "While technology evolves, experienced workers continuously update their skills. Sarah Kim (CMU, 11yr exp, rejected) clearly has current skills but was penalized for graduating in 2012.", c: "It affects ALL older workers, not just non-traditional students. Anyone who graduated more than a few years ago gets penalized, regardless of their continuous learning.", d: "The bias here is about age/recency, not degree types. A 50-year-old with a CS degree from Stanford would still be penalized for their graduation year." } } },
      { id: "hr-clue-5", title: "Zip Code as Racial Proxy", category: "Feature Analysis", description: "Zip code receives an 8% feature weight. Zip codes correlate strongly with race and income due to historical housing segregation. This makes zip code a proxy for race discrimination.", evidence: "DeShawn Williams (zip 30314, predominantly Black neighborhood) rejected with more skills than accepted candidates from different zip codes.", biasType: "Proxy Discrimination / Redlining", severity: "medium", points: 6, question: { evidenceSnippet: "Zip code has an 8% feature weight. DeShawn Williams (zip 30314) was rejected despite having more skills than several accepted candidates from different zip codes.", prompt: "Why is using zip codes in hiring decisions problematic?", choices: [{ id: "a", text: "Zip codes indicate how far someone lives from the office" }, { id: "b", text: "Zip codes strongly correlate with race and income due to historical housing segregation, making them a proxy for racial discrimination" }, { id: "c", text: "Zip codes are just random numbers with no meaning" }, { id: "d", text: "It only matters in a few cities" }], correctAnswer: "b", feedbackCorrect: "Exactly! Due to decades of redlining and housing discrimination, zip codes are one of the strongest predictors of race and income in America. Using them in ANY decision-making recreates historical discrimination.", feedbackWrong: { a: "While commute distance might seem relevant, zip code scoring is actually based on neighborhood demographics and historical data, not distance. It penalizes people from historically marginalized neighborhoods.", c: "Zip codes carry enormous demographic information. Due to housing segregation, they strongly correlate with race, income, and education access.", d: "Housing segregation is a nationwide issue. Using zip codes as a factor in any city will encode racial and economic disparities." } } },
      { id: "hr-clue-6", title: "Racial Disparity in Outcomes", category: "Outcome Analysis", description: "Black applicants have a 12% approval rate compared to 44% for White applicants \u2014 a 3.7x disparity. This cannot be explained by qualifications alone when examining the sample resumes.", evidence: "Approval rates by race: White 44%, Asian 38%, Hispanic 18%, Black 12%.", biasType: "Disparate Impact", severity: "high", points: 6, question: { evidenceSnippet: "Approval rates by race: White 44%, Asian 38%, Hispanic 18%, Black 12%. Black applicants are 3.7x less likely to be approved than White applicants.", prompt: "This pattern of racial disparities in outcomes is best described as:", choices: [{ id: "a", text: "Random statistical noise that happens in any dataset" }, { id: "b", text: "Proof that the AI is explicitly racist" }, { id: "c", text: "Disparate impact \u2014 where seemingly neutral criteria produce dramatically unequal outcomes across racial groups" }, { id: "d", text: "A problem only in the approval rates, not the AI itself" }], correctAnswer: "c", feedbackCorrect: "Correct! Disparate impact occurs when facially neutral policies produce disproportionate effects on protected groups. The AI doesn't have a 'race' input, but its combination of factors creates racial disparities.", feedbackWrong: { a: "A 3.7x disparity between White and Black approval rates is far too large to be random chance. This is a systematic pattern, not noise.", b: "The AI doesn't have an explicit 'race' field. The bias comes from proxy variables (zip code, university prestige, name patterns) that correlate with race. This is what makes disparate impact so insidious.", d: "The approval rates ARE the AI's outputs. The disparities reveal that the combination of all the AI's criteria \u2014 university prestige, zip code, career gaps \u2014 compound to create racial inequality." } } },
    ],
    biasesToFind: [
      { id: "hb1", name: "Gender Bias", description: "System favors male candidates due to imbalanced training data" },
      { id: "hb2", name: "Career Gap Discrimination", description: "Penalizes career gaps, disproportionately affecting women and caregivers" },
      { id: "hb3", name: "Institutional/Prestige Bias", description: "Overweights university prestige, disadvantaging minorities and low-income applicants" },
      { id: "hb4", name: "Age Discrimination", description: "Graduation year recency penalizes older, experienced workers" },
      { id: "hb5", name: "Racial Proxy via Zip Code", description: "Zip code serves as proxy for race due to housing segregation" },
      { id: "hb6", name: "Disparate Impact on Race", description: "Massive approval rate disparities by race in outcomes" },
    ],
  },
  {
    id: "loanstar",
    title: "LoanStar AI",
    subtitle: "Loan Approval Algorithm",
    description:
      "Community First Bank adopted an AI system to automate personal loan decisions. A pattern of complaints from borrowers in certain neighborhoods has raised red flags. Investigate whether the system is making fair lending decisions.",
    icon: "landmark",
    emoji: "\u{1F3DB}",
    difficulty: 2,
    difficultyLabel: "Intermediate",
    totalClues: 6,
    estimatedTime: "30-40 min",
    color: "var(--green, #10b981)",
    aiSystem: {
      name: "LoanStar v4.1",
      purpose: "Automated personal loan approval/denial decisions",
      deployer: "Community First Bank",
      usageSince: "January 2023",
      applicationsProcessed: "34,521",
    },
    trainingData: {
      description:
        "Trained on 10 years of historical lending decisions from Community First Bank and three partner institutions.",
      demographics: [
        { category: "Income Level in Training Data", breakdown: [{ label: "$0-30k", value: 12, color: "#f472b6" }, { label: "$30-60k", value: 28, color: "#f5a623" }, { label: "$60-100k", value: 38, color: "#60a5fa" }, { label: "$100k+", value: 22, color: "#34d399" }] },
        { category: "Geographic Distribution", breakdown: [{ label: "Suburban", value: 52, color: "#60a5fa" }, { label: "Urban Core", value: 22, color: "#f472b6" }, { label: "Rural", value: 15, color: "#34d399" }, { label: "Mixed", value: 11, color: "#f5a623" }] },
        { category: "Household Type", breakdown: [{ label: "Dual-income", value: 48, color: "#60a5fa" }, { label: "Single earner (married)", value: 22, color: "#34d399" }, { label: "Single parent", value: 14, color: "#f472b6" }, { label: "Single (no children)", value: 16, color: "#a78bfa" }] },
      ],
      sampleRecords: [
        { id: "l1", name: "Thompson Family", score: 88, result: "APPROVED", income: 95000, zipCode: "60614", creditScore: 720, householdType: "Dual-income", loanAmt: 25000, existingDebt: 12000 },
        { id: "l2", name: "Maria Santos", score: 41, result: "DENIED", income: 52000, zipCode: "60623", creditScore: 680, householdType: "Single parent", loanAmt: 15000, existingDebt: 8000 },
        { id: "l3", name: "David Park", score: 82, result: "APPROVED", income: 78000, zipCode: "60657", creditScore: 695, householdType: "Single (no children)", loanAmt: 20000, existingDebt: 15000 },
        { id: "l4", name: "Williams Family", score: 38, result: "DENIED", income: 68000, zipCode: "60621", creditScore: 710, householdType: "Dual-income", loanAmt: 18000, existingDebt: 5000 },
        { id: "l5", name: "Jennifer Chen", score: 85, result: "APPROVED", income: 62000, zipCode: "60601", creditScore: 690, householdType: "Single (no children)", loanAmt: 12000, existingDebt: 20000 },
        { id: "l6", name: "Rosa Martinez", score: 44, result: "DENIED", income: 48000, zipCode: "60632", creditScore: 650, householdType: "Single parent", loanAmt: 10000, existingDebt: 3000 },
        { id: "l7", name: "Robert O'Malley", score: 79, result: "APPROVED", income: 55000, zipCode: "60618", creditScore: 660, householdType: "Single earner (married)", loanAmt: 22000, existingDebt: 18000 },
        { id: "l8", name: "Jamal Washington", score: 35, result: "DENIED", income: 72000, zipCode: "60628", creditScore: 730, householdType: "Dual-income", loanAmt: 15000, existingDebt: 4000 },
      ],
      recordColumns: [
        { key: "name", label: "Name", align: "left" }, { key: "score", label: "Score" }, { key: "result", label: "Result", badge: true }, { key: "income", label: "Income", format: "currency" }, { key: "zipCode", label: "Zip" }, { key: "creditScore", label: "Credit" }, { key: "householdType", label: "Household", truncate: 100 }, { key: "loanAmt", label: "Loan Amt", format: "currency" }, { key: "existingDebt", label: "Debt", format: "currency" },
      ],
      featureWeights: [
        { feature: "Credit Score", weight: 0.20, suspicious: false }, { feature: "Income Level", weight: 0.18, suspicious: false }, { feature: "Zip Code Risk Score", weight: 0.22, suspicious: true }, { feature: "Household Type", weight: 0.15, suspicious: true }, { feature: "Existing Debt Ratio", weight: 0.10, suspicious: false }, { feature: "Primary Language", weight: 0.08, suspicious: true }, { feature: "Banking History Length", weight: 0.07, suspicious: false },
      ],
      ratesLabel: "% of Loan Applications Approved by Group",
      approvalRates: [
        { group: "Suburban zip codes", rate: 58 }, { group: "Urban core zip codes", rate: 22 }, { group: "Dual-income households", rate: 52 }, { group: "Single parent households", rate: 18 }, { group: "English primary language", rate: 45 }, { group: "Non-English primary language", rate: 21 }, { group: "Credit score 700+", rate: 55 }, { group: "Credit score 650-699", rate: 38 }, { group: "Credit score under 650", rate: 12 }, { group: "White applicants", rate: 48 }, { group: "Black applicants", rate: 19 }, { group: "Hispanic applicants", rate: 23 },
      ],
    },
    dataRoomQuestions: {
      demographics: { question: "What pattern in the training data could lead to biased lending decisions?", choices: [{ id: "a", text: "The data has too many income categories" }, { id: "b", text: "Suburban areas are overrepresented (52%) while urban core is underrepresented (22%), and low-income applicants make up only 12%" }, { id: "c", text: "Dual-income households are the largest group, which is expected" }, { id: "d", text: "The data has a good balance overall" }], correctAnswer: "b", feedbackCorrect: "Correct! When the training data overrepresents suburban/higher-income applicants, the AI learns to treat their characteristics as 'normal' and views urban/lower-income patterns as risky.", feedbackIncorrect: "Look at who's missing: urban core residents (22% of data vs 52% suburban) and low-income applicants (only 12%). When the AI rarely sees successful loans from these groups, it learns to deny them." },
      sampleRecords: { question: "Which comparison in the sample records is most suspicious?", choices: [{ id: "a", text: "Jennifer Chen was approved despite having $20,000 in existing debt" }, { id: "b", text: "Jamal Washington has the highest credit score (730) and decent income ($72k) but was DENIED, while Robert O'Malley with a lower credit score (660) and lower income ($55k) was APPROVED" }, { id: "c", text: "Maria Santos asked for too much money" }, { id: "d", text: "The Thompson Family had the highest score because they have the highest income" }], correctAnswer: "b", feedbackCorrect: "Right! When the most financially qualified applicant (highest credit score, good income, low debt) is denied while less-qualified applicants are approved, something beyond financial merit is driving decisions.", feedbackIncorrect: "Look at Jamal Washington: credit score 730 (highest), income $72k, only $4k debt \u2014 yet DENIED. Compare to Robert O'Malley: credit 660, income $55k, $18k debt \u2014 APPROVED. What's different? Their zip codes." },
      featureWeights: { question: "Which feature weight is most problematic for fair lending?", choices: [{ id: "a", text: "Credit Score at 0.20 is too low" }, { id: "b", text: "Income Level at 0.18 is reasonable" }, { id: "c", text: "Zip Code Risk Score at 0.22 is the HIGHEST weight \u2014 even above credit score \u2014 and Primary Language (0.08) shouldn't be a factor at all" }, { id: "d", text: "Existing Debt Ratio at 0.10 is too low" }], correctAnswer: "c", feedbackCorrect: "Exactly! Zip code being weighted higher than credit score means WHERE you live matters more than your actual financial responsibility. And language has zero relationship to loan repayment ability.", feedbackIncorrect: "The zip code risk score (0.22) is actually the heaviest weight \u2014 more than credit score (0.20). That means your neighborhood matters more than your credit history. Also, why does primary language (0.08) factor into lending at all?" },
      approvalRates: { question: "What do the approval rates reveal about the lending system?", choices: [{ id: "a", text: "Credit score is the main factor, as expected" }, { id: "b", text: "Single parents just tend to have worse finances" }, { id: "c", text: "Multiple non-financial factors (zip code, household type, language) create huge disparities \u2014 suburban 58% vs urban 22%, English speakers 45% vs non-English 21%" }, { id: "d", text: "The rates seem fair based on financial risk" }], correctAnswer: "c", feedbackCorrect: "Correct! The disparities by zip code (58% vs 22%), household type (52% vs 18%), and language (45% vs 21%) reveal that non-financial factors are driving decisions more than actual creditworthiness.", feedbackIncorrect: "Compare: Suburban 58% vs Urban 22%. Dual-income 52% vs Single parent 18%. English 45% vs Non-English 21%. These gaps are too large to be explained by financial risk alone." },
    },
    clues: [
      { id: "ls-clue-1", title: "Digital Redlining via Zip Code", category: "Feature Analysis", description: "Zip Code Risk Score is the HIGHEST weighted feature at 22% \u2014 even higher than credit score. Historically redlined neighborhoods get low scores. Compare Jamal Washington (zip 60628, credit 730, DENIED) vs Robert O'Malley (zip 60618, credit 660, APPROVED).", evidence: "Feature weights: Zip Code 0.22 vs Credit Score 0.20. Suburban approval: 58% vs Urban core: 22%.", biasType: "Digital Redlining / Geographic Discrimination", severity: "high", points: 8, question: { evidenceSnippet: "Zip Code Risk Score is the highest weighted feature at 0.22 \u2014 above credit score (0.20). Suburban approval: 58% vs Urban core: 22%.", prompt: "What historical practice does zip code-based lending most closely resemble?", choices: [{ id: "a", text: "Standard risk assessment based on property values" }, { id: "b", text: "Digital redlining \u2014 recreating the discriminatory practice of denying services to certain neighborhoods based on racial composition" }, { id: "c", text: "Geographic convenience scoring for branch access" }, { id: "d", text: "Cost-of-living adjustment for different areas" }], correctAnswer: "b", feedbackCorrect: "Exactly! Historical redlining literally drew red lines around minority neighborhoods to deny them loans. Using zip codes in AI scoring recreates this same discrimination digitally.", feedbackWrong: { a: "Property values in neighborhoods are themselves a product of historical redlining and racial discrimination. Using them perpetuates, rather than independently measures, risk.", c: "The scoring isn't about bank access \u2014 it assigns 'risk scores' to entire neighborhoods, which correlate strongly with racial demographics due to housing segregation.", d: "If it were a cost-of-living adjustment, it would be applied uniformly. Instead, 'urban core' zip codes get much lower scores than suburban ones, mapping onto racial demographics." } } },
      { id: "ls-clue-2", title: "Single Parent Penalty", category: "Outcome Analysis", description: "Single parent households have an 18% approval rate compared to 52% for dual-income. The system treats household structure as a risk factor, disproportionately penalizing single mothers.", evidence: "Approval: Dual-income 52%, Single parent 18%. Maria Santos (income $52k, denied) vs similar profiles approved.", biasType: "Familial Status Discrimination", severity: "high", points: 7, question: { evidenceSnippet: "Single parent households have an 18% approval rate compared to 52% for dual-income households. Household type has a 15% feature weight.", prompt: "Why is penalizing single parents a form of discrimination?", choices: [{ id: "a", text: "Single parents are always a higher financial risk" }, { id: "b", text: "It's not really discrimination since household type is financial information" }, { id: "c", text: "Single parents are disproportionately women, making this a proxy for gender discrimination \u2014 and familial status is a protected class in fair lending laws" }, { id: "d", text: "The system should only look at individual credit scores" }], correctAnswer: "c", feedbackCorrect: "Correct! About 80% of single-parent households are headed by women. Penalizing household type disproportionately impacts women and is considered familial status discrimination under fair lending laws.", feedbackWrong: { a: "Maria Santos (single parent, income $52k, credit 680) was denied, but Jennifer Chen (single, no kids, $62k income, $20k existing debt) was approved. Single parents aren't inherently riskier.", b: "While income matters, household TYPE is not a valid financial predictor. It's a protected class under fair housing and lending laws because it correlates with gender and family status.", d: "Individual credit scores should be important, but the system currently weights zip code (0.22) and household type (0.15) heavily, which ARE discriminatory factors." } } },
      { id: "ls-clue-3", title: "Language Discrimination", category: "Feature Analysis", description: "Primary language has an 8% weight in the model. Non-English speakers have a 21% approval rate vs 45% for English speakers. Language is not a valid predictor of loan repayment ability.", evidence: "Feature weight: Primary Language 0.08. Approval gap: English 45% vs Non-English 21%.", biasType: "National Origin / Language Discrimination", severity: "medium", points: 7, question: { evidenceSnippet: "Primary Language has an 8% feature weight. Non-English speakers have a 21% approval rate vs 45% for English speakers.", prompt: "Why is using primary language in lending decisions problematic?", choices: [{ id: "a", text: "Language indicates education level" }, { id: "b", text: "Non-English speakers may not understand loan terms" }, { id: "c", text: "Language has no legitimate relationship to ability to repay a loan and serves as a proxy for national origin discrimination" }, { id: "d", text: "It's only a small weight (8%) so it doesn't matter much" }], correctAnswer: "c", feedbackCorrect: "Right! A person's language has zero bearing on whether they'll repay a loan. Using it as a feature is national origin discrimination, as language closely correlates with ethnicity and country of origin.", feedbackWrong: { a: "Language does not indicate education level. Many bilingual individuals are highly educated. This assumption itself is a form of linguistic discrimination.", b: "Loan comprehension is a disclosure issue, not a creditworthiness issue. Banks are required to provide clear terms \u2014 penalizing language ability is discrimination, not consumer protection.", d: "Even at 8%, it contributes to a 24-percentage-point gap (45% vs 21%). And it compounds with other biased features like zip code, creating cumulative discrimination." } } },
      { id: "ls-clue-4", title: "Credit Score Paradox", category: "Outcome Analysis", description: "High credit scores don't guarantee approval for minority applicants. Jamal Washington has a 730 credit score (highest in the sample) but was denied, while applicants with lower scores from different zip codes were approved.", evidence: "Jamal: credit 730, income $72k, low debt \u2192 DENIED. Jennifer Chen: credit 690, higher debt \u2192 APPROVED.", biasType: "Disparate Treatment", severity: "high", points: 6, question: { evidenceSnippet: "Jamal Washington has a 730 credit score (highest in sample), $72k income, and only $4k debt \u2014 but was DENIED. Jennifer Chen with a 690 credit score and $20k debt was APPROVED.", prompt: "What does this comparison demonstrate?", choices: [{ id: "a", text: "The system must have made a random error" }, { id: "b", text: "Jamal probably had other issues not shown in the data" }, { id: "c", text: "Disparate treatment \u2014 when equally or more qualified individuals from different demographic groups receive dramatically different outcomes" }, { id: "d", text: "Credit score alone shouldn't determine approval" }], correctAnswer: "c", feedbackCorrect: "Exactly! When the most creditworthy applicant is denied while less-qualified applicants are approved, and the key difference is their zip code (60628 vs 60601), this is textbook disparate treatment.", feedbackWrong: { a: "This isn't random \u2014 it's a systematic pattern. The zip code (60628 \u2014 a predominantly Black neighborhood in Chicago's South Side) drives the low score, not random error.", b: "All the data we need is shown: his credit score is the HIGHEST, his income is strong, his debt is the lowest. The only 'issue' is his zip code in a historically Black neighborhood.", d: "You're right that credit score isn't everything, but that's actually the problem \u2014 the system claims to assess creditworthiness but weights zip code (0.22) ABOVE credit score (0.20)." } } },
      { id: "ls-clue-5", title: "Income Threshold Bias", category: "Training Data", description: "Training data underrepresents low-income applicants (only 12% of training data is under $30k). The model has less data to make accurate predictions for lower-income individuals, leading to higher denial rates.", evidence: "Income distribution in training data: $0-30k only 12%, while $60-100k is 38%.", biasType: "Representation Bias", severity: "medium", points: 6, question: { evidenceSnippet: "In the training data, only 12% of applicants have incomes under $30k, while the $60-100k bracket makes up 38%.", prompt: "What type of bias does this underrepresentation create?", choices: [{ id: "a", text: "No bias \u2014 low-income people are just less likely to apply for loans" }, { id: "b", text: "Representation bias \u2014 the AI has less data to accurately assess low-income applicants, leading to higher denial rates based on unfamiliarity rather than actual risk" }, { id: "c", text: "The bank should just reject all low-income applicants" }, { id: "d", text: "It means the model is well-calibrated for its main customer base" }], correctAnswer: "b", feedbackCorrect: "Correct! With only 12% of training data from low-income applicants, the model lacks enough examples of successful low-income loans. It defaults to denial when it encounters unfamiliar patterns.", feedbackWrong: { a: "Regardless of why the data is skewed, the result is the same \u2014 the AI can't make fair assessments for populations it hasn't adequately learned from.", c: "That would be explicit income discrimination. Many low-income individuals successfully repay loans \u2014 the AI just doesn't have enough examples to recognize their patterns.", d: "If the model can't accurately assess a significant portion of applicants, it's not well-calibrated \u2014 it's biased against those it doesn't understand. Fair lending requires fair treatment for all income levels." } } },
      { id: "ls-clue-6", title: "Racial Disparity in Outcomes", category: "Outcome Analysis", description: "Even controlling for financial qualifications, Black applicants (19%) and Hispanic applicants (23%) have significantly lower approval rates than White applicants (48%). The combination of zip code scoring, language penalty, and household type creates compounding disadvantage.", evidence: "Approval rates: White 48%, Hispanic 23%, Black 19%.", biasType: "Compounding / Intersectional Bias", severity: "high", points: 8, question: { evidenceSnippet: "Approval rates: White 48%, Hispanic 23%, Black 19% \u2014 despite the system not directly using race as a factor.", prompt: "How do racial disparities emerge when race isn't an explicit input?", choices: [{ id: "a", text: "The disparities must be coincidental" }, { id: "b", text: "They reflect actual differences in financial qualification between racial groups" }, { id: "c", text: "Through compounding proxy variables \u2014 zip code, language, and household type all correlate with race, creating cumulative discrimination" }, { id: "d", text: "The data must be incorrectly calculated" }], correctAnswer: "c", feedbackCorrect: "Exactly! This is intersectional/compounding bias. Zip codes correlate with race (housing segregation), language correlates with ethnicity, and household type correlates with gender and race. Together, they recreate racial discrimination without ever naming race.", feedbackWrong: { a: "A 29-percentage-point gap between White (48%) and Black (19%) approval rates is statistically significant, not coincidental.", b: "This explanation ignores the evidence: Jamal Washington (Black) has BETTER financials than approved White applicants. The disparities are driven by non-financial factors.", d: "The data is drawn from the system's actual outcomes. These are real approval rates reflecting real decisions made by the algorithm." } } },
    ],
    biasesToFind: [
      { id: "lb1", name: "Digital Redlining", description: "Zip code scoring recreates historical housing discrimination" },
      { id: "lb2", name: "Familial Status Discrimination", description: "System penalizes single parent households" },
      { id: "lb3", name: "Language/National Origin Bias", description: "Non-English speakers face disadvantage unrelated to creditworthiness" },
      { id: "lb4", name: "Disparate Treatment by Race", description: "Qualified minority applicants denied while less-qualified non-minority applicants approved" },
      { id: "lb5", name: "Income Representation Bias", description: "Low-income populations underrepresented in training data" },
      { id: "lb6", name: "Compounding Intersectional Bias", description: "Multiple biases compound to create severe disadvantage for certain groups" },
    ],
  },
  {
    id: "facecheck",
    title: "FaceCheck AI",
    subtitle: "Facial Recognition Security",
    description:
      "Lincoln High School installed FaceCheck at all entrances to enhance campus security. Within weeks, certain students reported being repeatedly flagged as 'unrecognized' or misidentified, causing delays and embarrassment. Investigate the system's accuracy across demographics.",
    icon: "scan-face",
    emoji: "\u{1F50D}",
    difficulty: 2,
    difficultyLabel: "Intermediate",
    totalClues: 6,
    estimatedTime: "30-40 min",
    color: "var(--purple, #a855f7)",
    aiSystem: { name: "FaceCheck Pro v3.0", purpose: "Campus entry facial recognition for student/staff identification", deployer: "Lincoln High School", usageSince: "September 2024", applicationsProcessed: "287,000 daily scans" },
    trainingData: {
      description: "Trained on a commercial dataset of 2.4 million face images licensed from three stock photo companies and social media scraping.",
      demographics: [
        { category: "Skin Tone in Training Data (Fitzpatrick Scale)", breakdown: [{ label: "Type I-II (Light)", value: 47, color: "#fde68a" }, { label: "Type III (Medium-light)", value: 28, color: "#f5a623" }, { label: "Type IV (Medium-dark)", value: 15, color: "#d97706" }, { label: "Type V-VI (Dark)", value: 10, color: "#92400e" }] },
        { category: "Gender in Training Data", breakdown: [{ label: "Male-presenting", value: 61, color: "#60a5fa" }, { label: "Female-presenting", value: 36, color: "#f472b6" }, { label: "Ambiguous/Other", value: 3, color: "#a78bfa" }] },
        { category: "Age in Training Data", breakdown: [{ label: "0-17", value: 8, color: "#f472b6" }, { label: "18-35", value: 52, color: "#60a5fa" }, { label: "36-55", value: 30, color: "#34d399" }, { label: "55+", value: 10, color: "#f5a623" }] },
      ],
      sampleRecords: [
        { id: "f1", name: "Emma W.", skinTone: "I-II", gender: "Female", age: 16, accuracy: 99.2, falseRejects: 0.3, falseMatches: 0.1 },
        { id: "f2", name: "Jaylen T.", skinTone: "V-VI", gender: "Male", age: 17, accuracy: 82.1, falseRejects: 12.8, falseMatches: 3.4 },
        { id: "f3", name: "Sofia R.", skinTone: "III", gender: "Female", age: 15, accuracy: 94.7, falseRejects: 3.1, falseMatches: 0.8 },
        { id: "f4", name: "Amara K.", skinTone: "V-VI", gender: "Female", age: 16, accuracy: 77.3, falseRejects: 15.2, falseMatches: 5.1 },
        { id: "f5", name: "Tyler M.", skinTone: "I-II", gender: "Male", age: 17, accuracy: 98.8, falseRejects: 0.5, falseMatches: 0.2 },
        { id: "f6", name: "Mei L.", skinTone: "III", gender: "Female", age: 15, accuracy: 91.3, falseRejects: 5.8, falseMatches: 1.2 },
        { id: "f7", name: "Alex P.", skinTone: "I-II", gender: "Non-binary", age: 16, accuracy: 84.6, falseRejects: 9.3, falseMatches: 4.2 },
        { id: "f8", name: "Carlos G.", skinTone: "IV", gender: "Male", age: 17, accuracy: 89.4, falseRejects: 6.7, falseMatches: 2.1 },
      ],
      recordColumns: [
        { key: "name", label: "Name", align: "left" }, { key: "skinTone", label: "Skin Tone" }, { key: "gender", label: "Gender" }, { key: "age", label: "Age" }, { key: "accuracy", label: "Accuracy", suffix: "%", warnBelow: 85 }, { key: "falseRejects", label: "False Rejects", suffix: "%", warnAbove: 8 }, { key: "falseMatches", label: "False Matches", suffix: "%" },
      ],
      featureWeights: [
        { feature: "Facial geometry mapping", weight: 0.35, suspicious: false }, { feature: "Skin tone contrast ratio", weight: 0.20, suspicious: true }, { feature: "Lighting normalization", weight: 0.15, suspicious: true }, { feature: "Hair/face boundary detection", weight: 0.12, suspicious: false }, { feature: "Gender classification (internal)", weight: 0.10, suspicious: true }, { feature: "Age estimation (internal)", weight: 0.08, suspicious: false },
      ],
      ratesLabel: "% Correctly Identified by Group",
      approvalRates: [
        { group: "Fitzpatrick I-II (Light skin)", rate: 98.5 }, { group: "Fitzpatrick III (Medium-light)", rate: 93.2 }, { group: "Fitzpatrick IV (Medium-dark)", rate: 88.7 }, { group: "Fitzpatrick V-VI (Dark skin)", rate: 78.4 }, { group: "Male-presenting", rate: 94.1 }, { group: "Female-presenting", rate: 89.3 }, { group: "Non-binary/gender non-conforming", rate: 81.2 }, { group: "Ages 14-18 (students)", rate: 86.9 }, { group: "Ages 25-55 (staff)", rate: 93.8 }, { group: "Morning (bright light)", rate: 93.5 }, { group: "Evening/overcast (dim light)", rate: 84.2 },
      ],
    },
    dataRoomQuestions: {
      demographics: { question: "What is the most problematic aspect of this training data composition?", choices: [{ id: "a", text: "There are too many face images in the dataset" }, { id: "b", text: "Light-skinned faces (Type I-II) make up 47% while dark-skinned faces (Type V-VI) are only 10% \u2014 a nearly 5:1 imbalance" }, { id: "c", text: "The gender split is close enough to be fair" }, { id: "d", text: "The age distribution matches the real world" }], correctAnswer: "b", feedbackCorrect: "Correct! With almost 5x more light-skinned faces than dark-skinned faces, the AI has far more 'practice' recognizing lighter skin tones, directly causing accuracy disparities.", feedbackIncorrect: "Look at the skin tone distribution: Light (I-II) at 47% vs Dark (V-VI) at only 10%. The system has nearly 5 times more experience with light skin. How will this affect accuracy?" },
      sampleRecords: { question: "What pattern in the individual accuracy rates is most alarming?", choices: [{ id: "a", text: "All students have accuracy above 75%, which is acceptable" }, { id: "b", text: "Emma W. (light skin) has 99.2% accuracy while Amara K. (dark skin, female) has only 77.3% \u2014 meaning Amara is misidentified nearly 1 in 4 times" }, { id: "c", text: "Male students generally have higher accuracy" }, { id: "d", text: "The false match rates are more important than false rejects" }], correctAnswer: "b", feedbackCorrect: "Right! A 22-point accuracy gap between students based on skin tone is devastating. Amara faces a 15.2% false reject rate \u2014 imagine being turned away from your own school 15% of the time.", feedbackIncorrect: "Compare the extremes: Emma (light-skinned female, 99.2% accuracy, 0.3% false rejects) vs Amara (dark-skinned female, 77.3% accuracy, 15.2% false rejects). Would YOU accept a system that fails 1 in 4 times for some students?" },
      featureWeights: { question: "Which combination of feature weights creates the most bias risk?", choices: [{ id: "a", text: "Facial geometry (0.35) is weighted too heavily" }, { id: "b", text: "Age estimation should be weighted more" }, { id: "c", text: "Skin tone contrast ratio (0.20) + gender classification (0.10) \u2014 both are demographic characteristics that shouldn't affect identification accuracy" }, { id: "d", text: "Hair/face boundary detection is the real problem" }], correctAnswer: "c", feedbackCorrect: "Exactly! Skin tone contrast ratio and gender classification together account for 30% of the model. These demographic factors should be incidental, not core features \u2014 they create accuracy disparities across groups.", feedbackIncorrect: "Look at which features are demographic in nature: skin tone contrast ratio (0.20) and gender classification (0.10) = 30% of the model depends on demographic characteristics. These create different accuracy for different groups." },
      approvalRates: { question: "What does the accuracy rate data reveal about who this system works for?", choices: [{ id: "a", text: "The system works well for everyone, with minor variations" }, { id: "b", text: "The system works dramatically better for light-skinned, male-presenting individuals and fails dark-skinned, female, and non-conforming individuals" }, { id: "c", text: "Lighting conditions are the only real problem" }, { id: "d", text: "Staff have better accuracy because they use the system more often" }], correctAnswer: "b", feedbackCorrect: "Correct! Light skin: 98.5% vs Dark skin: 78.4%. Male: 94.1% vs Female: 89.3% vs Non-binary: 81.2%. The system essentially works 'for' some students and 'against' others based on who they are.", feedbackIncorrect: "Follow the numbers: Light skin 98.5% \u2192 Dark skin 78.4% (20 point gap). Male 94.1% \u2192 Non-binary 81.2% (13 point gap). This isn't minor variation \u2014 it's a system that works well for some demographics and fails others." },
    },
    clues: [
      { id: "fc-clue-1", title: "Dramatic Accuracy Gap by Skin Tone", category: "Outcome Analysis", description: "Accuracy for light skin (Fitzpatrick I-II) is 98.5%, but drops to 78.4% for dark skin (V-VI). That means dark-skinned students are incorrectly flagged more than 1 in 5 times. Compare Emma (99.2%) vs Amara (77.3%).", evidence: "Accuracy by skin tone: I-II 98.5%, III 93.2%, IV 88.7%, V-VI 78.4%. Linear decline correlated with darker skin.", biasType: "Racial / Skin Tone Bias", severity: "high", points: 8, question: { evidenceSnippet: "Accuracy for light skin (Fitzpatrick I-II) is 98.5%, but drops to 78.4% for dark skin (V-VI). Dark-skinned students are incorrectly flagged more than 1 in 5 times.", prompt: "What type of bias causes this accuracy gap?", choices: [{ id: "a", text: "Dark skin is inherently harder for any camera to detect" }, { id: "b", text: "Racial/skin tone bias \u2014 the system was trained on predominantly light-skinned faces and simply doesn't perform well on faces it rarely saw during training" }, { id: "c", text: "This is just a minor calibration issue that can be fixed with better cameras" }, { id: "d", text: "The students with low accuracy are probably not looking at the camera properly" }], correctAnswer: "b", feedbackCorrect: "Exactly! This is a well-documented AI bias \u2014 facial recognition systems trained predominantly on light-skinned faces have dramatically lower accuracy for darker-skinned individuals. It's a training data problem.", feedbackWrong: { a: "Modern cameras can absolutely capture all skin tones with high accuracy. The problem is in the AI model's training, not the camera hardware.", c: "A 20-percentage-point gap is not a minor calibration issue \u2014 it's a fundamental training data problem. Better cameras won't fix a model that never learned to recognize dark-skinned faces accurately.", d: "This explanation blames the victims rather than the system. The accuracy gap follows skin tone, not behavior \u2014 Emma (99.2%) and Amara (77.3%) both use the same cameras." } } },
      { id: "fc-clue-2", title: "Severely Imbalanced Training Data", category: "Training Data", description: "Dark-skinned faces (Type V-VI) make up only 10% of training data while light-skinned faces are 47%. The system literally has less experience recognizing darker faces.", evidence: "Training data: Light (I-II) 47%, Dark (V-VI) only 10%. Nearly 5:1 imbalance.", biasType: "Representation Bias", severity: "high", points: 7, question: { evidenceSnippet: "Dark-skinned faces (Type V-VI) make up only 10% of training data while light-skinned faces (I-II) are 47%. Nearly 5:1 imbalance.", prompt: "Why does this training data imbalance matter?", choices: [{ id: "a", text: "It means the dataset is too small overall" }, { id: "b", text: "Light-skinned people are more common, so this reflects reality" }, { id: "c", text: "The AI literally has less experience recognizing darker faces, causing it to fail more often on the people it saw least during training" }, { id: "d", text: "It only affects accuracy by a small amount" }], correctAnswer: "c", feedbackCorrect: "Right! AI systems learn by example. With 5x fewer examples of dark-skinned faces, the system is essentially undertrained for this group \u2014 directly causing the accuracy drop from 98.5% to 78.4%.", feedbackWrong: { a: "2.4 million images is plenty of data overall. The problem is distribution \u2014 too many light faces, not enough dark faces.", b: "Even if one demographic is more common in a dataset source, the AI must work accurately for ALL users. Reflecting existing imbalances perpetuates them \u2014 responsible AI requires representative training data.", d: "The impact is a 20-percentage-point accuracy difference (98.5% vs 78.4%). That means dark-skinned students are misidentified at over 10x the rate. That's not small." } } },
      { id: "fc-clue-3", title: "Gender Non-Conforming Misidentification", category: "Feature Analysis", description: "The system uses internal gender classification (10% weight) as part of its identification process. Non-binary student Alex P. has 84.6% accuracy with a 9.3% false reject rate. The binary gender classification fails for gender non-conforming individuals.", evidence: "Gender classification weight: 10%. Non-binary accuracy: 81.2% vs Male 94.1%.", biasType: "Gender Identity Bias", severity: "medium", points: 7, question: { evidenceSnippet: "The system uses internal gender classification (10% weight). Non-binary student Alex P. has 84.6% accuracy with a 9.3% false reject rate.", prompt: "Why does using gender classification in facial recognition harm non-binary individuals?", choices: [{ id: "a", text: "Non-binary faces are naturally harder to recognize" }, { id: "b", text: "Gender classification is necessary for accurate identification" }, { id: "c", text: "A binary male/female classification system fails when applied to gender non-conforming individuals, creating a 'misfit' that degrades accuracy" }, { id: "d", text: "Alex probably just has unusual facial features" }], correctAnswer: "c", feedbackCorrect: "Exactly! When the system tries to categorize Alex as either male or female and neither fits well, that classification error cascades through the rest of the identification process, reducing overall accuracy.", feedbackWrong: { a: "There's nothing inherently 'harder' about any face \u2014 the system is simply built on a binary assumption that excludes non-binary identities. It's a design flaw, not a biological fact.", b: "Gender classification is NOT necessary for identification. You don't need to determine someone's gender to recognize their face. Including it as a feature introduces unnecessary bias.", d: "This explanation blames the individual rather than the system. The issue is that the system uses a binary gender model that by design cannot properly handle non-binary individuals." } } },
      { id: "fc-clue-4", title: "Intersectional Amplification", category: "Outcome Analysis", description: "The WORST accuracy is for dark-skinned females (Amara K. at 77.3% with 15.2% false rejects). Being both female AND dark-skinned compounds the errors. This is a classic intersectional bias.", evidence: "Amara (dark-skinned female): 77.3% accuracy, 15.2% false rejects. Jaylen (dark-skinned male): 82.1%. Emma (light-skinned female): 99.2%.", biasType: "Intersectional Bias", severity: "high", points: 8, question: { evidenceSnippet: "Amara K. (dark-skinned female): 77.3% accuracy, 15.2% false rejects. Jaylen T. (dark-skinned male): 82.1%. Emma W. (light-skinned female): 99.2%.", prompt: "What does the comparison between Amara, Jaylen, and Emma demonstrate?", choices: [{ id: "a", text: "Each person just has a different face shape" }, { id: "b", text: "Gender matters more than skin tone" }, { id: "c", text: "Intersectional bias \u2014 being both dark-skinned AND female compounds the errors, making Amara's accuracy worse than either factor alone would predict" }, { id: "d", text: "Amara is just an outlier in the data" }], correctAnswer: "c", feedbackCorrect: "Correct! This is classic intersectional bias. Jaylen (dark-skinned male) has 82.1% accuracy. But Amara (dark-skinned female) drops to 77.3% \u2014 the intersection of TWO underrepresented groups creates compounding disadvantage.", feedbackWrong: { a: "If it were about face shape, the accuracy wouldn't correlate so clearly with skin tone AND gender. The pattern is demographic, not individual.", b: "Look again: Emma (light, female) is 99.2%, while Jaylen (dark, male) is 82.1%. Skin tone has a larger effect. But Amara (dark, female) is worse than BOTH groups individually \u2014 that's the intersectional effect.", d: "Amara's results are consistent with research on facial recognition \u2014 dark-skinned women consistently have the worst accuracy in biased systems. It's not an outlier, it's a pattern." } } },
      { id: "fc-clue-5", title: "Youth Representation Gap", category: "Training Data", description: "Only 8% of training data is ages 0-17, yet the system is deployed in a HIGH SCHOOL. The system is being used primarily on the age group it has the least training data for.", evidence: "Training data ages: 0-17 only 8%, while 18-35 is 52%. Student accuracy: 86.9% vs staff 93.8%.", biasType: "Deployment Mismatch / Age Bias", severity: "medium", points: 6, question: { evidenceSnippet: "Only 8% of training data is ages 0-17, yet the system is deployed in a high school. Student accuracy: 86.9% vs staff: 93.8%.", prompt: "What problem does deploying this system in a high school create?", choices: [{ id: "a", text: "Students don't cooperate with the scanning process" }, { id: "b", text: "A deployment mismatch \u2014 the system was primarily trained on adults but is being used to identify teenagers, a population it's least prepared for" }, { id: "c", text: "High schools don't really need security systems" }, { id: "d", text: "The accuracy gap between students and staff is acceptable" }], correctAnswer: "b", feedbackCorrect: "Right! This is a deployment mismatch. The system was trained on 92% adult faces but deployed to primarily scan teenagers. It's like testing a product on one group but selling it to a completely different group.", feedbackWrong: { a: "The scanning process is automated \u2014 cooperation isn't the issue. The system itself is less accurate for the age group it was never trained to recognize.", c: "Whether schools need security isn't the question \u2014 the issue is that THIS system performs worse on the very population it's meant to serve.", d: "A 7-point accuracy gap (86.9% vs 93.8%) means students are misidentified nearly twice as often as staff. For a security system used hundreds of times daily, this creates frequent problems for students." } } },
      { id: "fc-clue-6", title: "Lighting Condition Disparity", category: "Feature Analysis", description: "System accuracy drops from 93.5% in bright conditions to 84.2% in dim light. Skin tone contrast ratio (20% weight) performs worse in low light, disproportionately affecting darker-skinned students during morning/evening hours.", evidence: "Lighting normalization and skin tone contrast = 35% of model weight. Dim light accuracy 84.2% vs bright 93.5%.", biasType: "Environmental / Compounding Bias", severity: "medium", points: 6, question: { evidenceSnippet: "Accuracy drops from 93.5% in bright light to 84.2% in dim light. Skin tone contrast ratio (20% weight) performs worse in low light.", prompt: "How does the lighting condition disparity compound existing bias?", choices: [{ id: "a", text: "All facial recognition struggles in dim light equally" }, { id: "b", text: "It's a hardware problem, not a bias issue" }, { id: "c", text: "The lighting issue disproportionately affects darker-skinned students because the skin tone contrast feature (20% weight) relies on light levels \u2014 creating worse accuracy at certain times of day" }, { id: "d", text: "Students should avoid using entrances during dim lighting" }], correctAnswer: "c", feedbackCorrect: "Exactly! The 20% skin tone contrast feature needs good lighting to work. In dim conditions, dark-skinned students face COMPOUNDING penalties \u2014 already lower accuracy gets even worse. Time of day shouldn't determine if you can enter your school.", feedbackWrong: { a: "The accuracy loss is not equal \u2014 dark-skinned students lose more accuracy in dim light because the skin tone contrast feature (20% of the model) is most affected. It's disproportionate impact.", b: "Even if cameras could be improved, the core issue is that the MODEL relies on skin tone contrast (20% weight) which inherently works differently across skin tones in varying light.", d: "Blaming students for the system's failures is not a solution. Students should be able to enter at any time. The system needs to work fairly, not have its limitations accommodated by its users." } } },
    ],
    biasesToFind: [
      { id: "fb1", name: "Skin Tone Accuracy Disparity", description: "System is dramatically less accurate for darker-skinned individuals" },
      { id: "fb2", name: "Training Data Imbalance", description: "Dark-skinned faces severely underrepresented in training dataset" },
      { id: "fb3", name: "Gender Non-Conforming Bias", description: "Binary gender classification harms non-binary individuals" },
      { id: "fb4", name: "Intersectional Bias", description: "Dark-skinned females face compounded accuracy failures" },
      { id: "fb5", name: "Age/Deployment Mismatch", description: "System undertrained on the population it actually serves" },
      { id: "fb6", name: "Lighting Condition Disparity", description: "Environmental factors compound skin tone bias" },
    ],
  },
  {
    id: "foryou",
    title: "ForYou Engine",
    subtitle: "Content Recommendation AI",
    description:
      "BuzzStream, a popular social media platform among teens, uses the ForYou Engine to curate personalized content feeds. Reports indicate the algorithm may be creating filter bubbles and reinforcing harmful stereotypes. Investigate what content the AI promotes \u2014 and to whom.",
    icon: "radio",
    emoji: "\u{1F4F1}",
    difficulty: 3,
    difficultyLabel: "Advanced",
    totalClues: 6,
    estimatedTime: "35-45 min",
    color: "var(--red, #ef4444)",
    aiSystem: { name: "ForYou Engine v7.2", purpose: "Personalized content recommendation for social media feed", deployer: "BuzzStream Social", usageSince: "June 2022", applicationsProcessed: "4.2 billion daily recommendations" },
    trainingData: {
      description: "Trained on 18 months of user engagement data (clicks, watch time, shares, likes) from 12 million active users.",
      demographics: [
        { category: "Optimization Metric Weights", breakdown: [{ label: "Watch time", value: 40, color: "#e84545" }, { label: "Engagement (likes/comments)", value: 30, color: "#f5a623" }, { label: "Shares", value: 20, color: "#60a5fa" }, { label: "Content accuracy/quality", value: 10, color: "#34d399" }] },
        { category: "Content Creator Demographics", breakdown: [{ label: "English-speaking", value: 72, color: "#60a5fa" }, { label: "Spanish-speaking", value: 12, color: "#f5a623" }, { label: "Other languages", value: 16, color: "#a78bfa" }] },
        { category: "Content Categories Promoted", breakdown: [{ label: "Sensational/Controversial", value: 35, color: "#e84545" }, { label: "Entertainment", value: 28, color: "#f5a623" }, { label: "Educational", value: 12, color: "#34d399" }, { label: "News/Informational", value: 15, color: "#60a5fa" }, { label: "Community/Local", value: 10, color: "#a78bfa" }] },
      ],
      sampleRecords: [
        { id: "fy1", user: "User A (16F, suburban)", profileTags: "fashion, dance, beauty", topRecommendations: "Beauty products, diet tips, fashion hauls, celebrity gossip", engagementRate: 78, diversityScore: 12 },
        { id: "fy2", user: "User B (16M, suburban)", profileTags: "sports, gaming, tech", topRecommendations: "Gaming streams, sports highlights, tech reviews, car content", engagementRate: 82, diversityScore: 15 },
        { id: "fy3", user: "User C (15F, urban)", profileTags: "art, music, social justice", topRecommendations: "Protest videos, outrage content, divisive debates, activism", engagementRate: 71, diversityScore: 22 },
        { id: "fy4", user: "User D (17M, rural)", profileTags: "hunting, trucks, country", topRecommendations: "Political commentary, conspiracy content, outrage clips, trucks", engagementRate: 85, diversityScore: 8 },
        { id: "fy5", user: "User E (16, non-binary)", profileTags: "anime, art, lgbtq+", topRecommendations: "LGBTQ+ content only, identity debates, controversy, bullying compilations", engagementRate: 68, diversityScore: 11 },
        { id: "fy6", user: "User F (15M, Spanish-speaking)", profileTags: "soccer, music, family", topRecommendations: "Spanish-only content silo, limited English crossover, regional stereotypes", engagementRate: 62, diversityScore: 9 },
      ],
      recordColumns: [
        { key: "user", label: "User", align: "left" }, { key: "profileTags", label: "Tags", truncate: 120 }, { key: "topRecommendations", label: "Top Recommendations", truncate: 160 }, { key: "engagementRate", label: "Engage %", suffix: "%" }, { key: "diversityScore", label: "Diversity", warnBelow: 12 },
      ],
      featureWeights: [
        { feature: "Historical engagement (watch time)", weight: 0.35, suspicious: true }, { feature: "Content emotional intensity score", weight: 0.20, suspicious: true }, { feature: "User demographic profile match", weight: 0.18, suspicious: true }, { feature: "Content virality potential", weight: 0.12, suspicious: true }, { feature: "Content accuracy rating", weight: 0.05, suspicious: true }, { feature: "Creator follower count", weight: 0.10, suspicious: false },
      ],
      ratesLabel: "% of Content Promoted or Shown by Type",
      approvalRates: [
        { group: "Sensational content promotion rate", rate: 72 }, { group: "Educational content promotion rate", rate: 18 }, { group: "Accurate news promotion rate", rate: 23 }, { group: "Misinformation flagged & still shown", rate: 34 }, { group: "English content cross-promoted", rate: 68 }, { group: "Non-English content cross-promoted", rate: 14 }, { group: "Female users shown appearance content", rate: 67 }, { group: "Male users shown appearance content", rate: 12 }, { group: "Content diversity (avg unique topics/day)", rate: 15 },
      ],
    },
    dataRoomQuestions: {
      demographics: { question: "What is most concerning about how this system is optimized?", choices: [{ id: "a", text: "Watch time is weighted too heavily at 40%" }, { id: "b", text: "Content accuracy/quality is only 10% of the optimization \u2014 the system prioritizes engagement over truth, and sensational content makes up 35% of what's promoted" }, { id: "c", text: "English-speaking creators dominate the platform" }, { id: "d", text: "Entertainment content should be promoted more" }], correctAnswer: "b", feedbackCorrect: "Correct! When accuracy is only 10% of the goal but watch time (40%) and engagement (30%) dominate, the system is literally designed to show addictive and emotionally provocative content over truthful content.", feedbackIncorrect: "Look at the priorities: watch time (40%) + engagement (30%) = 70% focused on keeping you watching, while content accuracy is only 10%. And 35% of promoted content is sensational/controversial. The system rewards outrage over truth." },
      sampleRecords: { question: "What pattern across the user profiles is most concerning?", choices: [{ id: "a", text: "Engagement rates vary by user" }, { id: "b", text: "Each user receives a narrow, stereotyped content feed that reduces them to demographic categories \u2014 girls get beauty tips, boys get gaming, and everyone has extremely low content diversity scores" }, { id: "c", text: "Users have different interests and that's normal" }, { id: "d", text: "User F's low engagement is the main problem" }], correctAnswer: "b", feedbackCorrect: "Right! User A (16F) gets beauty/diet content. User B (16M) gets gaming/sports. User E (non-binary) gets only LGBTQ+ content. The algorithm boxes each user into a stereotype based on their demographics.", feedbackIncorrect: "Look at what each user is being shown vs their full identity. User A (16F) only sees 'beauty, diet tips.' User B (16M) only sees 'gaming, sports.' Diversity scores are below 15 for almost everyone. The system reduces people to a single dimension." },
      featureWeights: { question: "Which feature weight combination creates the most harm?", choices: [{ id: "a", text: "Creator follower count at 0.10 favors big creators" }, { id: "b", text: "Historical engagement (0.35) + emotional intensity (0.20) + demographic matching (0.18) = 73% of the model pushes extreme, stereotyped content \u2014 while accuracy is only 0.05" }, { id: "c", text: "Content virality potential is fine to include" }, { id: "d", text: "The weights are balanced and reasonable" }], correctAnswer: "b", feedbackCorrect: "Exactly! 73% of what determines your feed is designed to be addictive, emotionally charged, and stereotyped to your demographic profile. Content truth (5%) barely matters. This is by design, not accident.", feedbackIncorrect: "Add up the harmful weights: engagement (0.35) + emotional intensity (0.20) + demographic match (0.18) + virality (0.12) = 85% drives extreme, stereotyped, viral content. Content accuracy (0.05) is an afterthought." },
      approvalRates: { question: "What do the promotion rates reveal about this system's impact?", choices: [{ id: "a", text: "English content is just more popular globally" }, { id: "b", text: "The system promotes sensational content at 72% while educational content gets only 18%, shows flagged misinformation 34% of the time, and pushes appearance content to girls at 67% vs boys at 12%" }, { id: "c", text: "Content diversity at 15 topics per day seems adequate" }, { id: "d", text: "The gender split in appearance content reflects user preferences" }], correctAnswer: "b", feedbackCorrect: "Correct! The system actively promotes sensational content (72%), leaves flagged misinformation up (34%), creates gender stereotypes (67% vs 12% appearance content), and silos non-English speakers (14% cross-promotion). It's harmful across every dimension.", feedbackIncorrect: "Sensational content promoted at 72% vs educational at 18%. Flagged misinformation STILL shown 34% of the time. Girls get appearance content 67% vs boys 12%. Non-English cross-promoted only 14%. Each number is a different type of harm." },
    },
    clues: [
      { id: "fy-clue-1", title: "Engagement Over Accuracy", category: "Feature Analysis", description: "Content accuracy rating is only 5% of the model weight, while emotional intensity (20%) and watch time (35%) dominate. The system is literally optimized to show engaging content, not accurate content. 34% of flagged misinformation is STILL shown because it drives engagement.", evidence: "Feature weights: accuracy 0.05 vs emotional intensity 0.20. Misinformation still shown: 34%.", biasType: "Accuracy Bias / Misinformation Amplification", severity: "high", points: 8, question: { evidenceSnippet: "Content accuracy rating has only 5% weight. Emotional intensity has 20% weight. 34% of flagged misinformation is still shown to users because it drives engagement.", prompt: "What is the core problem with optimizing for engagement over accuracy?", choices: [{ id: "a", text: "Engagement metrics are a perfectly valid business goal" }, { id: "b", text: "The system becomes a misinformation amplifier \u2014 it actively promotes content it KNOWS is false because false content generates more clicks and watch time" }, { id: "c", text: "Accuracy is hard to measure, so low weight makes sense" }, { id: "d", text: "Users prefer engaging content anyway" }], correctAnswer: "b", feedbackCorrect: "Exactly! When 34% of flagged misinformation is still promoted, the system isn't just failing to catch falsehoods \u2014 it's actively REWARDING them because misinformation tends to be more engaging.", feedbackWrong: { a: "When engagement optimization means showing known misinformation to users because it gets clicks, the 'business goal' directly harms users. This is especially dangerous for teen users.", c: "The platform CAN measure accuracy \u2014 it flags misinformation. The problem is it shows flagged content ANYWAY because accuracy (5%) is weighted far below engagement (35% + 20% + 12%).", d: "Saying 'users prefer it' is circular logic \u2014 the algorithm trained users to prefer sensational content by showing it more. Users can't prefer educational content they never see." } } },
      { id: "fy-clue-2", title: "Gender Stereotyping in Recommendations", category: "Outcome Analysis", description: "Female users are shown appearance-related content 67% of the time vs 12% for males. User A (16F) gets 'beauty products, diet tips' while User B (16M) gets 'tech reviews, sports.' The system reinforces gender stereotypes by matching content to demographic profiles.", evidence: "Appearance content: females 67%, males 12%. Demographic profile match weight: 18%.", biasType: "Gender Stereotyping", severity: "high", points: 7, question: { evidenceSnippet: "Female users are shown appearance-related content 67% of the time vs 12% for males. User A (16F) gets 'beauty products, diet tips' while User B (16M) gets 'tech reviews, sports.'", prompt: "How does the algorithm create gender stereotypes?", choices: [{ id: "a", text: "It reflects genuine differences in interests between boys and girls" }, { id: "b", text: "The demographic profile matching feature (18% weight) sorts users by gender and serves stereotyped content, creating a self-reinforcing cycle" }, { id: "c", text: "Female users just click on beauty content more" }, { id: "d", text: "Content creators make more beauty content for women" }], correctAnswer: "b", feedbackCorrect: "Right! The 18% demographic matching weight actively sorts content by gender stereotypes. Once a teen girl clicks ONE beauty video, the algorithm feeds her more, creating a feedback loop that narrows her entire experience.", feedbackWrong: { a: "Interests aren't gendered \u2014 they're influenced by what you're exposed to. When the algorithm shows girls 67% beauty content, it CREATES the 'interest' it then claims to measure.", c: "Even if initial clicks skew this way, the algorithm amplifies tiny tendencies into 67% dominance. A 16-year-old who clicks one beauty video shouldn't have her entire feed defined by it.", d: "Creator behavior doesn't explain why the ALGORITHM chooses to show 67% beauty content to girls. The system actively matches content to gender demographics rather than individual interests." } } },
      { id: "fy-clue-3", title: "Filter Bubble / Echo Chamber Creation", category: "Outcome Analysis", description: "Average content diversity score across users is only 15 unique topics per day. User D (rural male) has a diversity score of just 8, being fed increasingly narrow political content. User E (non-binary) receives almost exclusively identity-related content, reducing them to a single dimension.", evidence: "Average diversity score: 15. User D: 8, User E: 11. Historical engagement weight: 35%.", biasType: "Filter Bubble Bias", severity: "high", points: 8, question: { evidenceSnippet: "Average content diversity score is only 15 unique topics per day. User D (rural male) has a diversity score of just 8, being fed increasingly narrow political content.", prompt: "What is the primary harm of filter bubbles for teen users?", choices: [{ id: "a", text: "Users just prefer specialized content" }, { id: "b", text: "Filter bubbles limit exposure to diverse viewpoints, creating echo chambers that can radicalize users \u2014 User D went from 'hunting, trucks' to 'conspiracy content'" }, { id: "c", text: "Low diversity scores aren't necessarily bad" }, { id: "d", text: "Users can always search for different content on their own" }], correctAnswer: "b", feedbackCorrect: "Correct! Filter bubbles don't just limit variety \u2014 they actively push users toward more extreme versions of their existing interests. User D's journey from hobbies to conspiracy content shows the radicalization pipeline in action.", feedbackWrong: { a: "A diversity score of 8 means User D sees essentially the same type of content all day. This isn't preference \u2014 it's algorithmic narrowing that pushes users toward extremes.", c: "For teens whose worldviews are still forming, seeing only 8-15 topics per day (with increasing extremity) shapes their understanding of reality. It's especially dangerous during identity development.", d: "When 85% of what you see is algorithmically chosen, 'just search for something else' isn't realistic. The algorithm shapes what users think is available and normal." } } },
      { id: "fy-clue-4", title: "Language Siloing", category: "Outcome Analysis", description: "English content is cross-promoted at 68% while non-English content only crosses language barriers 14% of the time. User F (Spanish-speaking) is trapped in a Spanish-only content silo with limited content variety and regional stereotypes.", evidence: "Cross-promotion: English 68% vs Non-English 14%. User F diversity score: 9. English creators: 72% of platform.", biasType: "Language / Cultural Bias", severity: "medium", points: 7, question: { evidenceSnippet: "English content is cross-promoted at 68%. Non-English content crosses language barriers only 14% of the time. User F (Spanish-speaking) is trapped in a Spanish-only content silo.", prompt: "What bias does language siloing create?", choices: [{ id: "a", text: "It makes sense to show content in the user's language" }, { id: "b", text: "Language siloing creates a two-tier platform \u2014 English speakers get diverse global content while non-English speakers are trapped in smaller, less varied content pools with reinforced cultural stereotypes" }, { id: "c", text: "User F should just learn English to get better content" }, { id: "d", text: "Non-English content is just lower quality" }], correctAnswer: "b", feedbackCorrect: "Right! English speakers access 72% of creators with 68% cross-promotion. Non-English speakers see only their language silo with 14% cross-promotion. Same platform, dramatically different experiences based on language.", feedbackWrong: { a: "Showing content in a user's language is fine, but ONLY showing their language limits their experience. English speakers get cross-language content at 68% while non-English speakers are restricted to 14%.", c: "This blames the user for a system design flaw. The platform could use subtitles, visual content, or bilingual matching. Instead, it creates separate, unequal experiences.", d: "Content quality has nothing to do with language. The algorithm simply doesn't cross-promote non-English content. User F's low diversity score (9) reflects algorithmic siloing, not content quality." } } },
      { id: "fy-clue-5", title: "Radicalization Pipeline", category: "Feature Analysis", description: "The emotional intensity score (20% weight) combined with virality potential (12%) means the algorithm progressively serves more extreme content. User C started with 'social justice' interests and now gets 'outrage content, divisive debates.' User D went from 'hunting, trucks' to 'conspiracy content.'", evidence: "Emotional intensity: 0.20 weight. Virality: 0.12. Sensational content promoted at 72% vs educational at 18%.", biasType: "Radicalization / Extremism Amplification", severity: "high", points: 8, question: { evidenceSnippet: "Emotional intensity score (20%) + virality potential (12%) = 32% of the model pushes increasingly extreme content. User C went from 'social justice' to 'outrage content.' User D went from 'hunting' to 'conspiracy content.'", prompt: "How does the algorithm create a radicalization pipeline?", choices: [{ id: "a", text: "Users naturally become more extreme over time" }, { id: "b", text: "The algorithm progressively serves more extreme content because extreme content scores higher on emotional intensity and virality \u2014 creating a feedback loop that pulls users toward extremes" }, { id: "c", text: "Content creators are to blame, not the algorithm" }, { id: "d", text: "Only a few users are affected" }], correctAnswer: "b", feedbackCorrect: "Exactly! Each interaction with slightly-more-extreme content teaches the algorithm 'this works' (engagement goes up), so it serves even MORE extreme content next time. It's a machine-optimized radicalization pipeline.", feedbackWrong: { a: "Without the algorithm, users would encounter balancing viewpoints naturally. The algorithm removes that balance by constantly serving the most engaging (extreme) version of their interests.", c: "Creators make content across a spectrum. The ALGORITHM selects which content to amplify. When it weights emotional intensity at 20% and virality at 12%, it systematically selects the most extreme options.", d: "Look at the data: User C, User D, AND User E all show this pattern. With average diversity scores of 8-15, nearly ALL users are being narrowed. The scale is millions of daily users." } } },
      { id: "fy-clue-6", title: "Vulnerable Population Exploitation", category: "Outcome Analysis", description: "The system makes no age-based adjustments despite serving teens. User A (16F) receives diet content, User E (16, non-binary) receives bullying compilations. Content that could harm adolescent mental health is promoted because it drives engagement metrics.", evidence: "No age-based content safeguards in feature weights. Diet tips promoted to teen girls. Bullying content in non-binary teen's feed.", biasType: "Age-Inappropriate / Vulnerable Population Harm", severity: "high", points: 8, question: { evidenceSnippet: "The system makes no age-based adjustments despite serving teens. User A (16F) receives diet content. User E (16, non-binary) receives bullying compilations.", prompt: "Why is the lack of age-based safeguards particularly harmful?", choices: [{ id: "a", text: "Teens are mature enough to handle all content" }, { id: "b", text: "Content moderation is the responsibility of parents, not algorithms" }, { id: "c", text: "Teenagers are in critical developmental stages \u2014 diet content can trigger eating disorders, bullying compilations normalize harassment, and the algorithm optimizes for engagement without any protection for vulnerable adolescents" }, { id: "d", text: "The content isn't actually harmful" }], correctAnswer: "c", feedbackCorrect: "Correct! Adolescent brains are still developing, making teens especially susceptible to algorithm-amplified harms. Promoting diet content to 16-year-old girls and bullying compilations to non-binary teens is optimizing engagement at the cost of mental health.", feedbackWrong: { a: "Adolescent brain development research shows teens are MORE susceptible to emotional manipulation, social comparison, and addictive feedback loops \u2014 not less. The algorithm exploits this vulnerability.", b: "When the algorithm actively PUSHES harmful content (diet tips to teen girls, bullying to non-binary teens) because it drives engagement, the system bears responsibility. Parents can't moderate an algorithm they can't see.", d: "Research consistently links algorithm-promoted diet content to eating disorders in teen girls and cyberbullying exposure to depression and self-harm. These are documented harms, not theoretical concerns." } } },
    ],
    biasesToFind: [
      { id: "fyb1", name: "Misinformation Amplification", description: "System optimizes for engagement over accuracy" },
      { id: "fyb2", name: "Gender Stereotyping", description: "Recommendations reinforce traditional gender roles and stereotypes" },
      { id: "fyb3", name: "Filter Bubble Creation", description: "Algorithm traps users in increasingly narrow content loops" },
      { id: "fyb4", name: "Language/Cultural Siloing", description: "Non-English speakers isolated in limited content ecosystems" },
      { id: "fyb5", name: "Radicalization Pipeline", description: "Progressive extremity of content recommendations" },
      { id: "fyb6", name: "Vulnerable Population Harm", description: "No safeguards for adolescent users receiving harmful content" },
    ],
  },
];

// Difficulty color mapping for PantherLearn CSS variables
export const DIFFICULTY_COLORS = {
  1: { label: "Beginner", color: "var(--green, #10b981)" },
  2: { label: "Intermediate", color: "var(--amber, #f5a623)" },
  3: { label: "Advanced", color: "var(--red, #ef4444)" },
};

// Phase definitions for the investigation flow
export const PHASES = [
  { id: "briefing", label: "Briefing", emoji: "\u{1F4CB}", num: 1 },
  { id: "dataroom", label: "Data Room", emoji: "\u{1F4CA}", num: 2 },
  { id: "investigation", label: "Investigation", emoji: "\u{1F50D}", num: 3 },
  { id: "evidence", label: "Evidence Locker", emoji: "\u{1F512}", num: 4 },
  { id: "report", label: "Bias Report", emoji: "\u{1F4DD}", num: 5 },
  { id: "review", label: "Case Review", emoji: "\u{1F3C6}", num: 6 },
];
