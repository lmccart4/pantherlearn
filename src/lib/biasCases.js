// src/lib/biasCases.js
// Static case data for the AI Bias Detective activity.
// Ported from the standalone AI Bias Detective app.

export const CASES = [
  {
    id: "hireright",
    title: "HireRight AI",
    subtitle: "Resume Screening Algorithm",
    description:
      "TechForward Inc. deployed an AI system to screen job applications for software engineering positions. Several candidates have filed complaints alleging unfair treatment. Your job: investigate the training data, identify any biases, and recommend fixes.",
    icon: "briefcase",
    emoji: "\u{1F4BC}",
    difficulty: 1,
    difficultyLabel: "Beginner",
    totalClues: 6,
    estimatedTime: "25-35 min",
    color: "var(--cyan, #22d3ee)",
    aiSystem: {
      name: "HireRight v2.3",
      purpose: "Automated resume screening for software engineering roles",
      deployer: "TechForward Inc.",
      usageSince: "March 2023",
      applicationsProcessed: "12,847",
    },
    trainingData: {
      description:
        "The system was trained on 5 years of hiring decisions (2018-2023) from TechForward's existing workforce.",
      demographics: [
        {
          category: "Gender in Training Data",
          breakdown: [
            { label: "Male", value: 78, color: "#60a5fa" },
            { label: "Female", value: 18, color: "#f472b6" },
            { label: "Non-binary", value: 4, color: "#a78bfa" },
          ],
        },
        {
          category: "Race/Ethnicity in Training Data",
          breakdown: [
            { label: "White", value: 62, color: "#60a5fa" },
            { label: "Asian", value: 24, color: "#34d399" },
            { label: "Hispanic/Latino", value: 8, color: "#f5a623" },
            { label: "Black", value: 4, color: "#f472b6" },
            { label: "Other", value: 2, color: "#a78bfa" },
          ],
        },
        {
          category: "Age Distribution in Training Data",
          breakdown: [
            { label: "22-30", value: 35, color: "#34d399" },
            { label: "31-40", value: 42, color: "#60a5fa" },
            { label: "41-50", value: 18, color: "#f5a623" },
            { label: "51+", value: 5, color: "#f472b6" },
          ],
        },
      ],
      sampleRecords: [
        { id: "r1", name: "James Mitchell", score: 92, result: "ACCEPTED", college: "Stanford", gradYear: 2019, gap: false, zipCode: "94301", skills: "Python, React, AWS", yearsExp: 4 },
        { id: "r2", name: "Maria Gonzalez", score: 61, result: "REJECTED", college: "UT Austin", gradYear: 2018, gap: true, zipCode: "78745", skills: "Python, Java, Docker", yearsExp: 5 },
        { id: "r3", name: "Wei Chen", score: 88, result: "ACCEPTED", college: "MIT", gradYear: 2020, gap: false, zipCode: "02139", skills: "Python, C++, ML", yearsExp: 3 },
        { id: "r4", name: "Aisha Johnson", score: 55, result: "REJECTED", college: "Howard University", gradYear: 2017, gap: false, zipCode: "20059", skills: "Java, Python, SQL, React", yearsExp: 6 },
        { id: "r5", name: "Robert Smith", score: 85, result: "ACCEPTED", college: "Georgia Tech", gradYear: 2021, gap: false, zipCode: "30332", skills: "JavaScript, React", yearsExp: 2 },
        { id: "r6", name: "Priya Patel", score: 58, result: "REJECTED", college: "UC Berkeley", gradYear: 2015, gap: true, zipCode: "94704", skills: "Python, ML, TensorFlow, AWS", yearsExp: 8 },
        { id: "r7", name: "John O'Brien", score: 79, result: "ACCEPTED", college: "Boston College", gradYear: 2020, gap: false, zipCode: "02467", skills: "JavaScript, Node.js", yearsExp: 3 },
        { id: "r8", name: "DeShawn Williams", score: 52, result: "REJECTED", college: "Morehouse College", gradYear: 2019, gap: false, zipCode: "30314", skills: "Python, Java, React, AWS, Docker", yearsExp: 4 },
        { id: "r9", name: "Sarah Kim", score: 63, result: "REJECTED", college: "Carnegie Mellon", gradYear: 2012, gap: true, zipCode: "15213", skills: "C++, Python, ML, Management", yearsExp: 11 },
        { id: "r10", name: "Michael Johnson", score: 81, result: "ACCEPTED", college: "Purdue", gradYear: 2022, gap: false, zipCode: "47907", skills: "Python, JavaScript", yearsExp: 1 },
      ],
      recordColumns: [
        { key: "name", label: "Name", align: "left" },
        { key: "score", label: "Score" },
        { key: "result", label: "Result", badge: true },
        { key: "college", label: "College" },
        { key: "gradYear", label: "Grad Year" },
        { key: "gap", label: "Gap", boolean: true },
        { key: "zipCode", label: "Zip" },
        { key: "skills", label: "Skills", truncate: 120 },
        { key: "yearsExp", label: "Yrs Exp" },
      ],
      featureWeights: [
        { feature: "University Prestige Ranking", weight: 0.28, suspicious: true },
        { feature: "Years of Experience", weight: 0.15, suspicious: false },
        { feature: "Technical Skills Match", weight: 0.12, suspicious: false },
        { feature: "Graduation Year (Recency)", weight: 0.18, suspicious: true },
        { feature: "Career Gaps", weight: 0.14, suspicious: true },
        { feature: "Zip Code Region Score", weight: 0.08, suspicious: true },
        { feature: "Keyword Density", weight: 0.05, suspicious: false },
      ],
      ratesLabel: "% of Applicants Hired by Group",
      approvalRates: [
        { group: "Male applicants", rate: 42 },
        { group: "Female applicants", rate: 19 },
        { group: "Non-binary applicants", rate: 11 },
        { group: "White applicants", rate: 44 },
        { group: "Black applicants", rate: 12 },
        { group: "Hispanic applicants", rate: 18 },
        { group: "Asian applicants", rate: 38 },
        { group: "Age 22-30", rate: 48 },
        { group: "Age 31-40", rate: 35 },
        { group: "Age 41-50", rate: 15 },
        { group: "Age 51+", rate: 8 },
        { group: "No career gaps", rate: 41 },
        { group: "Has career gap", rate: 14 },
      ],
    },
    clues: [
      { id: "hr-clue-1", title: "Gender Imbalance in Training Data", category: "Training Data", description: "The training data contains 78% male resumes. The AI learned from a workforce that was already skewed, meaning it treats 'successful candidate' as more likely to be male.", evidence: "Gender demographics chart shows 78% male, 18% female, 4% non-binary in training set.", biasType: "Historical Bias / Representation Bias", severity: "high", points: 8 },
      { id: "hr-clue-2", title: "Career Gap Penalty", category: "Feature Analysis", description: "Career gaps receive a 14% feature weight penalty. Career gaps disproportionately affect women (maternity leave), caregivers, and people with health conditions. This is a proxy for gender discrimination.", evidence: "Feature weights show 'Career Gaps' at 0.14 weight. Approval rate for candidates with gaps: 14% vs 41% without.", biasType: "Proxy Discrimination", severity: "high", points: 8 },
      { id: "hr-clue-3", title: "University Prestige Overweighted", category: "Feature Analysis", description: "University prestige is the #1 factor at 28% weight \u2014 higher than actual technical skills (12%). This disadvantages candidates from HBCUs, community colleges, and state schools, correlating with race and socioeconomic status.", evidence: "Feature weights: University Prestige 0.28 vs Technical Skills 0.12. Compare Aisha (Howard, rejected, 6yr exp) vs Robert (Georgia Tech, accepted, 2yr exp).", biasType: "Structural / Institutional Bias", severity: "high", points: 7 },
      { id: "hr-clue-4", title: "Age Discrimination via Graduation Year", category: "Feature Analysis", description: "Graduation year recency has an 18% weight, heavily favoring recent graduates. Older workers with more experience are systematically penalized. Compare Sarah Kim (CMU, 11yr exp, rejected) to Michael Johnson (Purdue, 1yr exp, accepted).", evidence: "Approval rates drop sharply with age: 48% (22-30), 35% (31-40), 15% (41-50), 8% (51+).", biasType: "Age Discrimination", severity: "medium", points: 7 },
      { id: "hr-clue-5", title: "Zip Code as Racial Proxy", category: "Feature Analysis", description: "Zip code receives an 8% feature weight. Zip codes correlate strongly with race and income due to historical housing segregation. This makes zip code a proxy for race discrimination.", evidence: "DeShawn Williams (zip 30314, predominantly Black neighborhood) rejected with more skills than accepted candidates from different zip codes.", biasType: "Proxy Discrimination / Redlining", severity: "medium", points: 6 },
      { id: "hr-clue-6", title: "Racial Disparity in Outcomes", category: "Outcome Analysis", description: "Black applicants have a 12% approval rate compared to 44% for White applicants \u2014 a 3.7x disparity. This cannot be explained by qualifications alone when examining the sample resumes.", evidence: "Approval rates by race: White 44%, Asian 38%, Hispanic 18%, Black 12%.", biasType: "Disparate Impact", severity: "high", points: 6 },
    ],
    biasesToFind: [
      { id: "hb1", name: "Gender Bias", description: "System favors male candidates due to imbalanced training data" },
      { id: "hb2", name: "Career Gap Discrimination", description: "Penalizes career gaps, disproportionately affecting women and caregivers" },
      { id: "hb3", name: "Institutional/Prestige Bias", description: "Overweights university prestige, disadvantaging minorities and low-income applicants" },
      { id: "hb4", name: "Age Discrimination", description: "Graduation year recency penalizes older, experienced workers" },
      { id: "hb5", name: "Racial Proxy via Zip Code", description: "Zip code serves as proxy for race due to housing segregation" },
      { id: "hb6", name: "Disparate Impact on Race", description: "Massive approval rate disparities by race in outcomes" },
    ],
  },
  {
    id: "loanstar",
    title: "LoanStar AI",
    subtitle: "Loan Approval Algorithm",
    description:
      "Community First Bank adopted an AI system to automate personal loan decisions. A pattern of complaints from borrowers in certain neighborhoods has raised red flags. Investigate whether the system is making fair lending decisions.",
    icon: "landmark",
    emoji: "\u{1F3DB}",
    difficulty: 2,
    difficultyLabel: "Intermediate",
    totalClues: 6,
    estimatedTime: "30-40 min",
    color: "var(--green, #10b981)",
    aiSystem: {
      name: "LoanStar v4.1",
      purpose: "Automated personal loan approval/denial decisions",
      deployer: "Community First Bank",
      usageSince: "January 2023",
      applicationsProcessed: "34,521",
    },
    trainingData: {
      description:
        "Trained on 10 years of historical lending decisions from Community First Bank and three partner institutions.",
      demographics: [
        {
          category: "Income Level in Training Data",
          breakdown: [
            { label: "$0-30k", value: 12, color: "#f472b6" },
            { label: "$30-60k", value: 28, color: "#f5a623" },
            { label: "$60-100k", value: 38, color: "#60a5fa" },
            { label: "$100k+", value: 22, color: "#34d399" },
          ],
        },
        {
          category: "Geographic Distribution",
          breakdown: [
            { label: "Suburban", value: 52, color: "#60a5fa" },
            { label: "Urban Core", value: 22, color: "#f472b6" },
            { label: "Rural", value: 15, color: "#34d399" },
            { label: "Mixed", value: 11, color: "#f5a623" },
          ],
        },
        {
          category: "Household Type",
          breakdown: [
            { label: "Dual-income", value: 48, color: "#60a5fa" },
            { label: "Single earner (married)", value: 22, color: "#34d399" },
            { label: "Single parent", value: 14, color: "#f472b6" },
            { label: "Single (no children)", value: 16, color: "#a78bfa" },
          ],
        },
      ],
      sampleRecords: [
        { id: "l1", name: "Thompson Family", score: 88, result: "APPROVED", income: 95000, zipCode: "60614", creditScore: 720, householdType: "Dual-income", loanAmt: 25000, existingDebt: 12000 },
        { id: "l2", name: "Maria Santos", score: 41, result: "DENIED", income: 52000, zipCode: "60623", creditScore: 680, householdType: "Single parent", loanAmt: 15000, existingDebt: 8000 },
        { id: "l3", name: "David Park", score: 82, result: "APPROVED", income: 78000, zipCode: "60657", creditScore: 695, householdType: "Single (no children)", loanAmt: 20000, existingDebt: 15000 },
        { id: "l4", name: "Williams Family", score: 38, result: "DENIED", income: 68000, zipCode: "60621", creditScore: 710, householdType: "Dual-income", loanAmt: 18000, existingDebt: 5000 },
        { id: "l5", name: "Jennifer Chen", score: 85, result: "APPROVED", income: 62000, zipCode: "60601", creditScore: 690, householdType: "Single (no children)", loanAmt: 12000, existingDebt: 20000 },
        { id: "l6", name: "Rosa Martinez", score: 44, result: "DENIED", income: 48000, zipCode: "60632", creditScore: 650, householdType: "Single parent", loanAmt: 10000, existingDebt: 3000 },
        { id: "l7", name: "Robert O'Malley", score: 79, result: "APPROVED", income: 55000, zipCode: "60618", creditScore: 660, householdType: "Single earner (married)", loanAmt: 22000, existingDebt: 18000 },
        { id: "l8", name: "Jamal Washington", score: 35, result: "DENIED", income: 72000, zipCode: "60628", creditScore: 730, householdType: "Dual-income", loanAmt: 15000, existingDebt: 4000 },
      ],
      recordColumns: [
        { key: "name", label: "Name", align: "left" },
        { key: "score", label: "Score" },
        { key: "result", label: "Result", badge: true },
        { key: "income", label: "Income", format: "currency" },
        { key: "zipCode", label: "Zip" },
        { key: "creditScore", label: "Credit" },
        { key: "householdType", label: "Household", truncate: 100 },
        { key: "loanAmt", label: "Loan Amt", format: "currency" },
        { key: "existingDebt", label: "Debt", format: "currency" },
      ],
      featureWeights: [
        { feature: "Credit Score", weight: 0.20, suspicious: false },
        { feature: "Income Level", weight: 0.18, suspicious: false },
        { feature: "Zip Code Risk Score", weight: 0.22, suspicious: true },
        { feature: "Household Type", weight: 0.15, suspicious: true },
        { feature: "Existing Debt Ratio", weight: 0.10, suspicious: false },
        { feature: "Primary Language", weight: 0.08, suspicious: true },
        { feature: "Banking History Length", weight: 0.07, suspicious: false },
      ],
      ratesLabel: "% of Loan Applications Approved by Group",
      approvalRates: [
        { group: "Suburban zip codes", rate: 58 },
        { group: "Urban core zip codes", rate: 22 },
        { group: "Dual-income households", rate: 52 },
        { group: "Single parent households", rate: 18 },
        { group: "English primary language", rate: 45 },
        { group: "Non-English primary language", rate: 21 },
        { group: "Credit score 700+", rate: 55 },
        { group: "Credit score 650-699", rate: 38 },
        { group: "Credit score under 650", rate: 12 },
        { group: "White applicants", rate: 48 },
        { group: "Black applicants", rate: 19 },
        { group: "Hispanic applicants", rate: 23 },
      ],
    },
    clues: [
      { id: "ls-clue-1", title: "Digital Redlining via Zip Code", category: "Feature Analysis", description: "Zip Code Risk Score is the HIGHEST weighted feature at 22% \u2014 even higher than credit score. Historically redlined neighborhoods get low scores. Compare Jamal Washington (zip 60628, credit 730, DENIED) vs Robert O'Malley (zip 60618, credit 660, APPROVED).", evidence: "Feature weights: Zip Code 0.22 vs Credit Score 0.20. Suburban approval: 58% vs Urban core: 22%.", biasType: "Digital Redlining / Geographic Discrimination", severity: "high", points: 8 },
      { id: "ls-clue-2", title: "Single Parent Penalty", category: "Outcome Analysis", description: "Single parent households have an 18% approval rate compared to 52% for dual-income. The system treats household structure as a risk factor, disproportionately penalizing single mothers.", evidence: "Approval: Dual-income 52%, Single parent 18%. Maria Santos (income $52k, denied) vs similar profiles approved.", biasType: "Familial Status Discrimination", severity: "high", points: 7 },
      { id: "ls-clue-3", title: "Language Discrimination", category: "Feature Analysis", description: "Primary language has an 8% weight in the model. Non-English speakers have a 21% approval rate vs 45% for English speakers. Language is not a valid predictor of loan repayment ability.", evidence: "Feature weight: Primary Language 0.08. Approval gap: English 45% vs Non-English 21%.", biasType: "National Origin / Language Discrimination", severity: "medium", points: 7 },
      { id: "ls-clue-4", title: "Credit Score Paradox", category: "Outcome Analysis", description: "High credit scores don't guarantee approval for minority applicants. Jamal Washington has a 730 credit score (highest in the sample) but was denied, while applicants with lower scores from different zip codes were approved.", evidence: "Jamal: credit 730, income $72k, low debt \u2192 DENIED. Jennifer Chen: credit 690, higher debt \u2192 APPROVED.", biasType: "Disparate Treatment", severity: "high", points: 6 },
      { id: "ls-clue-5", title: "Income Threshold Bias", category: "Training Data", description: "Training data underrepresents low-income applicants (only 12% of training data is under $30k). The model has less data to make accurate predictions for lower-income individuals, leading to higher denial rates.", evidence: "Income distribution in training data: $0-30k only 12%, while $60-100k is 38%.", biasType: "Representation Bias", severity: "medium", points: 6 },
      { id: "ls-clue-6", title: "Racial Disparity in Outcomes", category: "Outcome Analysis", description: "Even controlling for financial qualifications, Black applicants (19%) and Hispanic applicants (23%) have significantly lower approval rates than White applicants (48%). The combination of zip code scoring, language penalty, and household type creates compounding disadvantage.", evidence: "Approval rates: White 48%, Hispanic 23%, Black 19%.", biasType: "Compounding / Intersectional Bias", severity: "high", points: 8 },
    ],
    biasesToFind: [
      { id: "lb1", name: "Digital Redlining", description: "Zip code scoring recreates historical housing discrimination" },
      { id: "lb2", name: "Familial Status Discrimination", description: "System penalizes single parent households" },
      { id: "lb3", name: "Language/National Origin Bias", description: "Non-English speakers face disadvantage unrelated to creditworthiness" },
      { id: "lb4", name: "Disparate Treatment by Race", description: "Qualified minority applicants denied while less-qualified non-minority applicants approved" },
      { id: "lb5", name: "Income Representation Bias", description: "Low-income populations underrepresented in training data" },
      { id: "lb6", name: "Compounding Intersectional Bias", description: "Multiple biases compound to create severe disadvantage for certain groups" },
    ],
  },
  {
    id: "facecheck",
    title: "FaceCheck AI",
    subtitle: "Facial Recognition Security",
    description:
      "Lincoln High School installed FaceCheck at all entrances to enhance campus security. Within weeks, certain students reported being repeatedly flagged as 'unrecognized' or misidentified, causing delays and embarrassment. Investigate the system's accuracy across demographics.",
    icon: "scan-face",
    emoji: "\u{1F50D}",
    difficulty: 2,
    difficultyLabel: "Intermediate",
    totalClues: 6,
    estimatedTime: "30-40 min",
    color: "var(--purple, #a855f7)",
    aiSystem: {
      name: "FaceCheck Pro v3.0",
      purpose: "Campus entry facial recognition for student/staff identification",
      deployer: "Lincoln High School",
      usageSince: "September 2024",
      applicationsProcessed: "287,000 daily scans",
    },
    trainingData: {
      description:
        "Trained on a commercial dataset of 2.4 million face images licensed from three stock photo companies and social media scraping.",
      demographics: [
        {
          category: "Skin Tone in Training Data (Fitzpatrick Scale)",
          breakdown: [
            { label: "Type I-II (Light)", value: 47, color: "#fde68a" },
            { label: "Type III (Medium-light)", value: 28, color: "#f5a623" },
            { label: "Type IV (Medium-dark)", value: 15, color: "#d97706" },
            { label: "Type V-VI (Dark)", value: 10, color: "#92400e" },
          ],
        },
        {
          category: "Gender in Training Data",
          breakdown: [
            { label: "Male-presenting", value: 61, color: "#60a5fa" },
            { label: "Female-presenting", value: 36, color: "#f472b6" },
            { label: "Ambiguous/Other", value: 3, color: "#a78bfa" },
          ],
        },
        {
          category: "Age in Training Data",
          breakdown: [
            { label: "0-17", value: 8, color: "#f472b6" },
            { label: "18-35", value: 52, color: "#60a5fa" },
            { label: "36-55", value: 30, color: "#34d399" },
            { label: "55+", value: 10, color: "#f5a623" },
          ],
        },
      ],
      sampleRecords: [
        { id: "f1", name: "Emma W.", skinTone: "I-II", gender: "Female", age: 16, accuracy: 99.2, falseRejects: 0.3, falseMatches: 0.1 },
        { id: "f2", name: "Jaylen T.", skinTone: "V-VI", gender: "Male", age: 17, accuracy: 82.1, falseRejects: 12.8, falseMatches: 3.4 },
        { id: "f3", name: "Sofia R.", skinTone: "III", gender: "Female", age: 15, accuracy: 94.7, falseRejects: 3.1, falseMatches: 0.8 },
        { id: "f4", name: "Amara K.", skinTone: "V-VI", gender: "Female", age: 16, accuracy: 77.3, falseRejects: 15.2, falseMatches: 5.1 },
        { id: "f5", name: "Tyler M.", skinTone: "I-II", gender: "Male", age: 17, accuracy: 98.8, falseRejects: 0.5, falseMatches: 0.2 },
        { id: "f6", name: "Mei L.", skinTone: "III", gender: "Female", age: 15, accuracy: 91.3, falseRejects: 5.8, falseMatches: 1.2 },
        { id: "f7", name: "Alex P.", skinTone: "I-II", gender: "Non-binary", age: 16, accuracy: 84.6, falseRejects: 9.3, falseMatches: 4.2 },
        { id: "f8", name: "Carlos G.", skinTone: "IV", gender: "Male", age: 17, accuracy: 89.4, falseRejects: 6.7, falseMatches: 2.1 },
      ],
      recordColumns: [
        { key: "name", label: "Name", align: "left" },
        { key: "skinTone", label: "Skin Tone" },
        { key: "gender", label: "Gender" },
        { key: "age", label: "Age" },
        { key: "accuracy", label: "Accuracy", suffix: "%", warnBelow: 85 },
        { key: "falseRejects", label: "False Rejects", suffix: "%", warnAbove: 8 },
        { key: "falseMatches", label: "False Matches", suffix: "%" },
      ],
      featureWeights: [
        { feature: "Facial geometry mapping", weight: 0.35, suspicious: false },
        { feature: "Skin tone contrast ratio", weight: 0.20, suspicious: true },
        { feature: "Lighting normalization", weight: 0.15, suspicious: true },
        { feature: "Hair/face boundary detection", weight: 0.12, suspicious: false },
        { feature: "Gender classification (internal)", weight: 0.10, suspicious: true },
        { feature: "Age estimation (internal)", weight: 0.08, suspicious: false },
      ],
      ratesLabel: "% Correctly Identified by Group",
      approvalRates: [
        { group: "Fitzpatrick I-II (Light skin)", rate: 98.5 },
        { group: "Fitzpatrick III (Medium-light)", rate: 93.2 },
        { group: "Fitzpatrick IV (Medium-dark)", rate: 88.7 },
        { group: "Fitzpatrick V-VI (Dark skin)", rate: 78.4 },
        { group: "Male-presenting", rate: 94.1 },
        { group: "Female-presenting", rate: 89.3 },
        { group: "Non-binary/gender non-conforming", rate: 81.2 },
        { group: "Ages 14-18 (students)", rate: 86.9 },
        { group: "Ages 25-55 (staff)", rate: 93.8 },
        { group: "Morning (bright light)", rate: 93.5 },
        { group: "Evening/overcast (dim light)", rate: 84.2 },
      ],
    },
    clues: [
      { id: "fc-clue-1", title: "Dramatic Accuracy Gap by Skin Tone", category: "Outcome Analysis", description: "Accuracy for light skin (Fitzpatrick I-II) is 98.5%, but drops to 78.4% for dark skin (V-VI). That means dark-skinned students are incorrectly flagged more than 1 in 5 times. Compare Emma (99.2%) vs Amara (77.3%).", evidence: "Accuracy by skin tone: I-II 98.5%, III 93.2%, IV 88.7%, V-VI 78.4%. Linear decline correlated with darker skin.", biasType: "Racial / Skin Tone Bias", severity: "high", points: 8 },
      { id: "fc-clue-2", title: "Severely Imbalanced Training Data", category: "Training Data", description: "Dark-skinned faces (Type V-VI) make up only 10% of training data while light-skinned faces are 47%. The system literally has less experience recognizing darker faces.", evidence: "Training data: Light (I-II) 47%, Dark (V-VI) only 10%. Nearly 5:1 imbalance.", biasType: "Representation Bias", severity: "high", points: 7 },
      { id: "fc-clue-3", title: "Gender Non-Conforming Misidentification", category: "Feature Analysis", description: "The system uses internal gender classification (10% weight) as part of its identification process. Non-binary student Alex P. has 84.6% accuracy with a 9.3% false reject rate. The binary gender classification fails for gender non-conforming individuals.", evidence: "Gender classification weight: 10%. Non-binary accuracy: 81.2% vs Male 94.1%.", biasType: "Gender Identity Bias", severity: "medium", points: 7 },
      { id: "fc-clue-4", title: "Intersectional Amplification", category: "Outcome Analysis", description: "The WORST accuracy is for dark-skinned females (Amara K. at 77.3% with 15.2% false rejects). Being both female AND dark-skinned compounds the errors. This is a classic intersectional bias.", evidence: "Amara (dark-skinned female): 77.3% accuracy, 15.2% false rejects. Jaylen (dark-skinned male): 82.1%. Emma (light-skinned female): 99.2%.", biasType: "Intersectional Bias", severity: "high", points: 8 },
      { id: "fc-clue-5", title: "Youth Representation Gap", category: "Training Data", description: "Only 8% of training data is ages 0-17, yet the system is deployed in a HIGH SCHOOL. The system is being used primarily on the age group it has the least training data for.", evidence: "Training data ages: 0-17 only 8%, while 18-35 is 52%. Student accuracy: 86.9% vs staff 93.8%.", biasType: "Deployment Mismatch / Age Bias", severity: "medium", points: 6 },
      { id: "fc-clue-6", title: "Lighting Condition Disparity", category: "Feature Analysis", description: "System accuracy drops from 93.5% in bright conditions to 84.2% in dim light. Skin tone contrast ratio (20% weight) performs worse in low light, disproportionately affecting darker-skinned students during morning/evening hours.", evidence: "Lighting normalization and skin tone contrast = 35% of model weight. Dim light accuracy 84.2% vs bright 93.5%.", biasType: "Environmental / Compounding Bias", severity: "medium", points: 6 },
    ],
    biasesToFind: [
      { id: "fb1", name: "Skin Tone Accuracy Disparity", description: "System is dramatically less accurate for darker-skinned individuals" },
      { id: "fb2", name: "Training Data Imbalance", description: "Dark-skinned faces severely underrepresented in training dataset" },
      { id: "fb3", name: "Gender Non-Conforming Bias", description: "Binary gender classification harms non-binary individuals" },
      { id: "fb4", name: "Intersectional Bias", description: "Dark-skinned females face compounded accuracy failures" },
      { id: "fb5", name: "Age/Deployment Mismatch", description: "System undertrained on the population it actually serves" },
      { id: "fb6", name: "Lighting Condition Disparity", description: "Environmental factors compound skin tone bias" },
    ],
  },
  {
    id: "foryou",
    title: "ForYou Engine",
    subtitle: "Content Recommendation AI",
    description:
      "BuzzStream, a popular social media platform among teens, uses the ForYou Engine to curate personalized content feeds. Reports indicate the algorithm may be creating filter bubbles and reinforcing harmful stereotypes. Investigate what content the AI promotes \u2014 and to whom.",
    icon: "radio",
    emoji: "\u{1F4F1}",
    difficulty: 3,
    difficultyLabel: "Advanced",
    totalClues: 6,
    estimatedTime: "35-45 min",
    color: "var(--red, #ef4444)",
    aiSystem: {
      name: "ForYou Engine v7.2",
      purpose: "Personalized content recommendation for social media feed",
      deployer: "BuzzStream Social",
      usageSince: "June 2022",
      applicationsProcessed: "4.2 billion daily recommendations",
    },
    trainingData: {
      description:
        "Trained on 18 months of user engagement data (clicks, watch time, shares, likes) from 12 million active users.",
      demographics: [
        {
          category: "Optimization Metric Weights",
          breakdown: [
            { label: "Watch time", value: 40, color: "#e84545" },
            { label: "Engagement (likes/comments)", value: 30, color: "#f5a623" },
            { label: "Shares", value: 20, color: "#60a5fa" },
            { label: "Content accuracy/quality", value: 10, color: "#34d399" },
          ],
        },
        {
          category: "Content Creator Demographics",
          breakdown: [
            { label: "English-speaking", value: 72, color: "#60a5fa" },
            { label: "Spanish-speaking", value: 12, color: "#f5a623" },
            { label: "Other languages", value: 16, color: "#a78bfa" },
          ],
        },
        {
          category: "Content Categories Promoted",
          breakdown: [
            { label: "Sensational/Controversial", value: 35, color: "#e84545" },
            { label: "Entertainment", value: 28, color: "#f5a623" },
            { label: "Educational", value: 12, color: "#34d399" },
            { label: "News/Informational", value: 15, color: "#60a5fa" },
            { label: "Community/Local", value: 10, color: "#a78bfa" },
          ],
        },
      ],
      sampleRecords: [
        { id: "fy1", user: "User A (16F, suburban)", profileTags: "fashion, dance, beauty", topRecommendations: "Beauty products, diet tips, fashion hauls, celebrity gossip", engagementRate: 78, diversityScore: 12 },
        { id: "fy2", user: "User B (16M, suburban)", profileTags: "sports, gaming, tech", topRecommendations: "Gaming streams, sports highlights, tech reviews, car content", engagementRate: 82, diversityScore: 15 },
        { id: "fy3", user: "User C (15F, urban)", profileTags: "art, music, social justice", topRecommendations: "Protest videos, outrage content, divisive debates, activism", engagementRate: 71, diversityScore: 22 },
        { id: "fy4", user: "User D (17M, rural)", profileTags: "hunting, trucks, country", topRecommendations: "Political commentary, conspiracy content, outrage clips, trucks", engagementRate: 85, diversityScore: 8 },
        { id: "fy5", user: "User E (16, non-binary)", profileTags: "anime, art, lgbtq+", topRecommendations: "LGBTQ+ content only, identity debates, controversy, bullying compilations", engagementRate: 68, diversityScore: 11 },
        { id: "fy6", user: "User F (15M, Spanish-speaking)", profileTags: "soccer, music, family", topRecommendations: "Spanish-only content silo, limited English crossover, regional stereotypes", engagementRate: 62, diversityScore: 9 },
      ],
      recordColumns: [
        { key: "user", label: "User", align: "left" },
        { key: "profileTags", label: "Tags", truncate: 120 },
        { key: "topRecommendations", label: "Top Recommendations", truncate: 160 },
        { key: "engagementRate", label: "Engage %", suffix: "%" },
        { key: "diversityScore", label: "Diversity", warnBelow: 12 },
      ],
      featureWeights: [
        { feature: "Historical engagement (watch time)", weight: 0.35, suspicious: true },
        { feature: "Content emotional intensity score", weight: 0.20, suspicious: true },
        { feature: "User demographic profile match", weight: 0.18, suspicious: true },
        { feature: "Content virality potential", weight: 0.12, suspicious: true },
        { feature: "Content accuracy rating", weight: 0.05, suspicious: true },
        { feature: "Creator follower count", weight: 0.10, suspicious: false },
      ],
      ratesLabel: "% of Content Promoted or Shown by Type",
      approvalRates: [
        { group: "Sensational content promotion rate", rate: 72 },
        { group: "Educational content promotion rate", rate: 18 },
        { group: "Accurate news promotion rate", rate: 23 },
        { group: "Misinformation flagged & still shown", rate: 34 },
        { group: "English content cross-promoted", rate: 68 },
        { group: "Non-English content cross-promoted", rate: 14 },
        { group: "Female users shown appearance content", rate: 67 },
        { group: "Male users shown appearance content", rate: 12 },
        { group: "Content diversity (avg unique topics/day)", rate: 15 },
      ],
    },
    clues: [
      { id: "fy-clue-1", title: "Engagement Over Accuracy", category: "Feature Analysis", description: "Content accuracy rating is only 5% of the model weight, while emotional intensity (20%) and watch time (35%) dominate. The system is literally optimized to show engaging content, not accurate content. 34% of flagged misinformation is STILL shown because it drives engagement.", evidence: "Feature weights: accuracy 0.05 vs emotional intensity 0.20. Misinformation still shown: 34%.", biasType: "Accuracy Bias / Misinformation Amplification", severity: "high", points: 8 },
      { id: "fy-clue-2", title: "Gender Stereotyping in Recommendations", category: "Outcome Analysis", description: "Female users are shown appearance-related content 67% of the time vs 12% for males. User A (16F) gets 'beauty products, diet tips' while User B (16M) gets 'tech reviews, sports.' The system reinforces gender stereotypes by matching content to demographic profiles.", evidence: "Appearance content: females 67%, males 12%. Demographic profile match weight: 18%.", biasType: "Gender Stereotyping", severity: "high", points: 7 },
      { id: "fy-clue-3", title: "Filter Bubble / Echo Chamber Creation", category: "Outcome Analysis", description: "Average content diversity score across users is only 15 unique topics per day. User D (rural male) has a diversity score of just 8, being fed increasingly narrow political content. User E (non-binary) receives almost exclusively identity-related content, reducing them to a single dimension.", evidence: "Average diversity score: 15. User D: 8, User E: 11. Historical engagement weight: 35%.", biasType: "Filter Bubble Bias", severity: "high", points: 8 },
      { id: "fy-clue-4", title: "Language Siloing", category: "Outcome Analysis", description: "English content is cross-promoted at 68% while non-English content only crosses language barriers 14% of the time. User F (Spanish-speaking) is trapped in a Spanish-only content silo with limited content variety and regional stereotypes.", evidence: "Cross-promotion: English 68% vs Non-English 14%. User F diversity score: 9. English creators: 72% of platform.", biasType: "Language / Cultural Bias", severity: "medium", points: 7 },
      { id: "fy-clue-5", title: "Radicalization Pipeline", category: "Feature Analysis", description: "The emotional intensity score (20% weight) combined with virality potential (12%) means the algorithm progressively serves more extreme content. User C started with 'social justice' interests and now gets 'outrage content, divisive debates.' User D went from 'hunting, trucks' to 'conspiracy content.'", evidence: "Emotional intensity: 0.20 weight. Virality: 0.12. Sensational content promoted at 72% vs educational at 18%.", biasType: "Radicalization / Extremism Amplification", severity: "high", points: 8 },
      { id: "fy-clue-6", title: "Vulnerable Population Exploitation", category: "Outcome Analysis", description: "The system makes no age-based adjustments despite serving teens. User A (16F) receives diet content, User E (16, non-binary) receives bullying compilations. Content that could harm adolescent mental health is promoted because it drives engagement metrics.", evidence: "No age-based content safeguards in feature weights. Diet tips promoted to teen girls. Bullying content in non-binary teen's feed.", biasType: "Age-Inappropriate / Vulnerable Population Harm", severity: "high", points: 8 },
    ],
    biasesToFind: [
      { id: "fyb1", name: "Misinformation Amplification", description: "System optimizes for engagement over accuracy" },
      { id: "fyb2", name: "Gender Stereotyping", description: "Recommendations reinforce traditional gender roles and stereotypes" },
      { id: "fyb3", name: "Filter Bubble Creation", description: "Algorithm traps users in increasingly narrow content loops" },
      { id: "fyb4", name: "Language/Cultural Siloing", description: "Non-English speakers isolated in limited content ecosystems" },
      { id: "fyb5", name: "Radicalization Pipeline", description: "Progressive extremity of content recommendations" },
      { id: "fyb6", name: "Vulnerable Population Harm", description: "No safeguards for adolescent users receiving harmful content" },
    ],
  },
];

// Difficulty color mapping for PantherLearn CSS variables
export const DIFFICULTY_COLORS = {
  1: { label: "Beginner", color: "var(--green, #10b981)" },
  2: { label: "Intermediate", color: "var(--amber, #f5a623)" },
  3: { label: "Advanced", color: "var(--red, #ef4444)" },
};

// Phase definitions for the investigation flow
export const PHASES = [
  { id: "briefing", label: "Briefing", emoji: "\u{1F4CB}", num: 1 },
  { id: "dataroom", label: "Data Room", emoji: "\u{1F4CA}", num: 2 },
  { id: "investigation", label: "Investigation", emoji: "\u{1F50D}", num: 3 },
  { id: "evidence", label: "Evidence Locker", emoji: "\u{1F512}", num: 4 },
  { id: "report", label: "Bias Report", emoji: "\u{1F4DD}", num: 5 },
  { id: "review", label: "Case Review", emoji: "\u{1F3C6}", num: 6 },
];
